{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJSkS1KoEVAH"
   },
   "source": [
    "### WGAN\n",
    "\n",
    "* Модифицируйте код ячеек ниже и реализуйте [Wasserstein GAN](https://arxiv.org/abs/1701.07875) с клиппингом весов. (10 баллов)\n",
    "\n",
    "* Замените клиппинг весов на [штраф градентов](https://arxiv.org/pdf/1704.00028v3.pdf). (10 баллов)\n",
    "\n",
    "* Добавьте лейблы в WGAN, тем самым решая задачу [условной генерации](https://arxiv.org/pdf/1411.1784.pdf). (30 баллов)\n",
    "\n",
    "Добавьте в этот файл анализ полученных результатов с различными графиками обучения и визуализацию генерации. Сравните как работает клиппинг весов и штраф градиентов и попробуйте пронаблюдать какие недостатки имеет модель GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_PBZhPXoEVAI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA1WUmq7EVAI"
   },
   "source": [
    "### Простой конфиг (для хранения параметров, можете использовать и модифицировать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6ZOVySNaEVAJ"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    pass\n",
    "\n",
    "config = Config()\n",
    "config.mnist_path = None\n",
    "config.batch_size = 16\n",
    "config.num_workers = 3\n",
    "config.num_epochs = 10\n",
    "config.noise_size = 50\n",
    "config.print_freq = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4z2spNOEVAJ"
   },
   "source": [
    "### Создаем dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rJQdF3-xEVAJ"
   },
   "outputs": [],
   "source": [
    "train = torchvision.datasets.FashionMNIST(\"fashion_mnist\", train=True, transform=torchvision.transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DsFXiFsIEVAJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3750"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GXozdEFQEVAJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = next(iter(dataloader))\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki7mSwhrEVAK"
   },
   "source": [
    "### Создаем модель GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "diozgmg4EVAK"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(config.noise_size, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 28*28),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(50, 1),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZJUbCECuEVAK"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUJaHpFbEVAL"
   },
   "source": [
    "### Оптимизатор и функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VltlxWoyEVAL"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "optim_G = optim.Adam(params=generator.parameters(), lr=0.0001)\n",
    "optim_D = optim.Adam(params=discriminator.parameters(), lr=0.0001)\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRbESDzYEVAL"
   },
   "source": [
    "### Для оптимизации процесса обучения можно заранее определить переменные и заполнять их значения новыми данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vXAHDeLlEVAL"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "legacy constructor expects device type: cpu but device type: cuda was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m noise \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mFloatTensor(config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mnoise_size, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# fixed_noise = Variable(torch.FloatTensor(config.batch_size, config.noise_size, device=device).normal_(0, 1))\u001b[39;00m\n\u001b[0;32m      3\u001b[0m label \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mFloatTensor(config\u001b[38;5;241m.\u001b[39mbatch_size, device\u001b[38;5;241m=\u001b[39mdevice))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: legacy constructor expects device type: cpu but device type: cuda was passed"
     ]
    }
   ],
   "source": [
    "noise = torch.randn(config.batch_size, config.noise_size, device=device)\n",
    "# fixed_noise = Variable(torch.FloatTensor(config.batch_size, config.noise_size, device=device).normal_(0, 1))\n",
    "label = torch.empty(config.batch_size, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3XozpMxEVAL"
   },
   "source": [
    "### GAN обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W_1xrPXEVAL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ERRD_x = np.zeros(config.num_epochs)\n",
    "ERRD_z = np.zeros(config.num_epochs)\n",
    "ERRG = np.zeros(config.num_epochs)\n",
    "N = len(dataloader)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    for iteration, (images, cat) in enumerate(dataloader):\n",
    "        #######\n",
    "        # Discriminator stage: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        #######\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # real\n",
    "        label.data.fill_(real_label)\n",
    "        input_data = images.view(images.shape[0], -1).to(device)\n",
    "        output = discriminator(input_data).view(-1)\n",
    "        errD_x = criterion(output, label)\n",
    "        ERRD_x[epoch] += errD_x.item()\n",
    "        errD_x.backward()\n",
    "\n",
    "        # fake\n",
    "        noise.data.normal_(0, 1)\n",
    "        fake = generator(noise)\n",
    "        label.data.fill_(fake_label)\n",
    "        output = discriminator(fake.detach()).view(-1)\n",
    "        errD_z = criterion(output, label)\n",
    "        ERRD_z[epoch] += errD_z.item()\n",
    "        errD_z.backward()\n",
    "\n",
    "        optim_D.step()\n",
    "\n",
    "        #######\n",
    "        # Generator stage: maximize log(D(G(x))\n",
    "        #######\n",
    "        generator.zero_grad()\n",
    "        label.data.fill_(real_label)\n",
    "        output = discriminator(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        ERRG[epoch] += errG.item()\n",
    "        errG.backward()\n",
    "\n",
    "        optim_G.step()\n",
    "\n",
    "        if (iteration+1) % config.print_freq == 0:\n",
    "            print('Epoch:{} Iter: {} errD_x: {:.2f} errD_z: {:.2f} errG: {:.2f}'.format(epoch+1,\n",
    "                                                                                            iteration+1,\n",
    "                                                                                            errD_x.item(),\n",
    "                                                                                            errD_z.item(),\n",
    "                                                                                            errG.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx7WhHRTEVAL"
   },
   "outputs": [],
   "source": [
    "noise.data.normal_(0, 1)\n",
    "fake = generator(noise)\n",
    "\n",
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(fake[i].detach().numpy().reshape(28, 28), cmap=plt.cm.Greys_r)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L23brfcxBHAZ"
   },
   "source": [
    "# Модифицируйте код ячеек ниже и реализуйте Wasserstein GAN с клиппингом весов. (10 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DcIBUddQBE70",
    "outputId": "abcc9d74-e579-4067-fcc3-ab4051a0b6ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NeKonn\\AppData\\Local\\Temp\\ipykernel_8308\\1874713943.py:80: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:85.)\n",
      "  z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], config.noise_size)))).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10] [Batch 0/3750] [D loss: -0.04200122505426407] [G loss: 0.01035439595580101]\n",
      "[Epoch 0/10] [Batch 100/3750] [D loss: -0.1731003224849701] [G loss: 0.08743385225534439]\n",
      "[Epoch 0/10] [Batch 200/3750] [D loss: -1.0171973705291748] [G loss: 0.8051367998123169]\n",
      "[Epoch 0/10] [Batch 300/3750] [D loss: -0.580783486366272] [G loss: -0.8433276414871216]\n",
      "[Epoch 0/10] [Batch 400/3750] [D loss: -0.011880559846758842] [G loss: 0.10061788558959961]\n",
      "[Epoch 0/10] [Batch 500/3750] [D loss: -0.03592371940612793] [G loss: -0.7771020531654358]\n",
      "[Epoch 0/10] [Batch 600/3750] [D loss: -0.2191803753376007] [G loss: -0.19149842858314514]\n",
      "[Epoch 0/10] [Batch 700/3750] [D loss: -0.04937458783388138] [G loss: 0.09418998658657074]\n",
      "[Epoch 0/10] [Batch 800/3750] [D loss: -0.39514318108558655] [G loss: -0.24274247884750366]\n",
      "[Epoch 0/10] [Batch 900/3750] [D loss: -0.4713520407676697] [G loss: 1.1858477592468262]\n",
      "[Epoch 0/10] [Batch 1000/3750] [D loss: -0.28361696004867554] [G loss: -0.028889410197734833]\n",
      "[Epoch 0/10] [Batch 1100/3750] [D loss: -0.8589558005332947] [G loss: -0.9582196474075317]\n",
      "[Epoch 0/10] [Batch 1200/3750] [D loss: -0.17556500434875488] [G loss: -0.602749228477478]\n",
      "[Epoch 0/10] [Batch 1300/3750] [D loss: -0.4955686330795288] [G loss: -0.4275217652320862]\n",
      "[Epoch 0/10] [Batch 1400/3750] [D loss: -0.4637363851070404] [G loss: -0.07080380618572235]\n",
      "[Epoch 0/10] [Batch 1500/3750] [D loss: -0.07812225818634033] [G loss: -0.2691091001033783]\n",
      "[Epoch 0/10] [Batch 1600/3750] [D loss: -0.5502532124519348] [G loss: -0.007938610389828682]\n",
      "[Epoch 0/10] [Batch 1700/3750] [D loss: -0.5138565897941589] [G loss: 0.09326576441526413]\n",
      "[Epoch 0/10] [Batch 1800/3750] [D loss: -0.13181966543197632] [G loss: -0.17341551184654236]\n",
      "[Epoch 0/10] [Batch 1900/3750] [D loss: -0.3332284092903137] [G loss: 0.645880937576294]\n",
      "[Epoch 0/10] [Batch 2000/3750] [D loss: -0.2759351432323456] [G loss: 0.8673837780952454]\n",
      "[Epoch 0/10] [Batch 2100/3750] [D loss: -0.09469230473041534] [G loss: 0.3054632544517517]\n",
      "[Epoch 0/10] [Batch 2200/3750] [D loss: -0.2250572144985199] [G loss: -0.32974863052368164]\n",
      "[Epoch 0/10] [Batch 2300/3750] [D loss: -0.47558125853538513] [G loss: -0.20896410942077637]\n",
      "[Epoch 0/10] [Batch 2400/3750] [D loss: -0.18239426612854004] [G loss: 0.017089705914258957]\n",
      "[Epoch 0/10] [Batch 2500/3750] [D loss: -0.30280351638793945] [G loss: 0.5645015835762024]\n",
      "[Epoch 0/10] [Batch 2600/3750] [D loss: -0.31468480825424194] [G loss: -0.04749975726008415]\n",
      "[Epoch 0/10] [Batch 2700/3750] [D loss: -0.03214821219444275] [G loss: -0.3131300210952759]\n",
      "[Epoch 0/10] [Batch 2800/3750] [D loss: -0.13876616954803467] [G loss: -0.109961599111557]\n",
      "[Epoch 0/10] [Batch 2900/3750] [D loss: -0.27458223700523376] [G loss: -0.08015897870063782]\n",
      "[Epoch 0/10] [Batch 3000/3750] [D loss: -0.34437456727027893] [G loss: 0.2570505738258362]\n",
      "[Epoch 0/10] [Batch 3100/3750] [D loss: -0.10482478141784668] [G loss: -0.35331350564956665]\n",
      "[Epoch 0/10] [Batch 3200/3750] [D loss: -0.19801320135593414] [G loss: 0.49807512760162354]\n",
      "[Epoch 0/10] [Batch 3300/3750] [D loss: -0.33555519580841064] [G loss: -0.1894870400428772]\n",
      "[Epoch 0/10] [Batch 3400/3750] [D loss: -0.36245912313461304] [G loss: 0.1256871521472931]\n",
      "[Epoch 0/10] [Batch 3500/3750] [D loss: -0.11286342144012451] [G loss: -0.5512115359306335]\n",
      "[Epoch 0/10] [Batch 3600/3750] [D loss: -0.3140326142311096] [G loss: -0.35242143273353577]\n",
      "[Epoch 0/10] [Batch 3700/3750] [D loss: -0.09566924721002579] [G loss: 0.18427306413650513]\n",
      "[Epoch 1/10] [Batch 0/3750] [D loss: -0.2701716721057892] [G loss: 0.3076157867908478]\n",
      "[Epoch 1/10] [Batch 100/3750] [D loss: -0.17875568568706512] [G loss: -0.2572360038757324]\n",
      "[Epoch 1/10] [Batch 200/3750] [D loss: -0.34769752621650696] [G loss: 0.35530638694763184]\n",
      "[Epoch 1/10] [Batch 300/3750] [D loss: -0.22436460852622986] [G loss: -0.1631002426147461]\n",
      "[Epoch 1/10] [Batch 400/3750] [D loss: -0.4359133839607239] [G loss: 0.42660844326019287]\n",
      "[Epoch 1/10] [Batch 500/3750] [D loss: -0.24021834135055542] [G loss: 0.3973234295845032]\n",
      "[Epoch 1/10] [Batch 600/3750] [D loss: 0.06000545620918274] [G loss: 0.12890809774398804]\n",
      "[Epoch 1/10] [Batch 700/3750] [D loss: -0.06493359804153442] [G loss: 0.35477060079574585]\n",
      "[Epoch 1/10] [Batch 800/3750] [D loss: -0.34389954805374146] [G loss: -1.0701696872711182]\n",
      "[Epoch 1/10] [Batch 900/3750] [D loss: -0.3318570852279663] [G loss: 0.3639374077320099]\n",
      "[Epoch 1/10] [Batch 1000/3750] [D loss: -0.07427794486284256] [G loss: 0.08994448184967041]\n",
      "[Epoch 1/10] [Batch 1100/3750] [D loss: -0.1621675044298172] [G loss: 0.24884329736232758]\n",
      "[Epoch 1/10] [Batch 1200/3750] [D loss: -0.20492082834243774] [G loss: 0.3837127089500427]\n",
      "[Epoch 1/10] [Batch 1300/3750] [D loss: -0.16003933548927307] [G loss: 0.6654753684997559]\n",
      "[Epoch 1/10] [Batch 1400/3750] [D loss: -0.09367966651916504] [G loss: 0.2967779040336609]\n",
      "[Epoch 1/10] [Batch 1500/3750] [D loss: -0.16471076011657715] [G loss: -0.3925929069519043]\n",
      "[Epoch 1/10] [Batch 1600/3750] [D loss: -0.10219689458608627] [G loss: -0.07719018310308456]\n",
      "[Epoch 1/10] [Batch 1700/3750] [D loss: -0.5573807954788208] [G loss: -0.32594212889671326]\n",
      "[Epoch 1/10] [Batch 1800/3750] [D loss: -0.11819517612457275] [G loss: 0.19211158156394958]\n",
      "[Epoch 1/10] [Batch 1900/3750] [D loss: 0.00827711820602417] [G loss: -0.1668339967727661]\n",
      "[Epoch 1/10] [Batch 2000/3750] [D loss: -0.5082059502601624] [G loss: 0.6662048697471619]\n",
      "[Epoch 1/10] [Batch 2100/3750] [D loss: -0.08758854866027832] [G loss: -0.38987216353416443]\n",
      "[Epoch 1/10] [Batch 2200/3750] [D loss: -0.14420513808727264] [G loss: 0.11980487406253815]\n",
      "[Epoch 1/10] [Batch 2300/3750] [D loss: -0.18754521012306213] [G loss: -0.3377857208251953]\n",
      "[Epoch 1/10] [Batch 2400/3750] [D loss: -0.11020106077194214] [G loss: -0.3569656312465668]\n",
      "[Epoch 1/10] [Batch 2500/3750] [D loss: -0.2446809709072113] [G loss: -0.3714006841182709]\n",
      "[Epoch 1/10] [Batch 2600/3750] [D loss: 0.02275848388671875] [G loss: -0.31919747591018677]\n",
      "[Epoch 1/10] [Batch 2700/3750] [D loss: -0.18070682883262634] [G loss: 0.6703767776489258]\n",
      "[Epoch 1/10] [Batch 2800/3750] [D loss: 0.03740943968296051] [G loss: 0.08905823528766632]\n",
      "[Epoch 1/10] [Batch 2900/3750] [D loss: -0.15266689658164978] [G loss: 0.7819682955741882]\n",
      "[Epoch 1/10] [Batch 3000/3750] [D loss: -0.278767466545105] [G loss: 0.3402656316757202]\n",
      "[Epoch 1/10] [Batch 3100/3750] [D loss: -0.09103848040103912] [G loss: 0.34032729268074036]\n",
      "[Epoch 1/10] [Batch 3200/3750] [D loss: -0.1005244180560112] [G loss: 0.06705603003501892]\n",
      "[Epoch 1/10] [Batch 3300/3750] [D loss: -0.18524229526519775] [G loss: -0.10295434296131134]\n",
      "[Epoch 1/10] [Batch 3400/3750] [D loss: 0.00041541457176208496] [G loss: 0.12725591659545898]\n",
      "[Epoch 1/10] [Batch 3500/3750] [D loss: 0.03628486394882202] [G loss: -0.31514257192611694]\n",
      "[Epoch 1/10] [Batch 3600/3750] [D loss: -0.447307825088501] [G loss: 1.5109879970550537]\n",
      "[Epoch 1/10] [Batch 3700/3750] [D loss: -0.10893717408180237] [G loss: 0.03494570031762123]\n",
      "[Epoch 2/10] [Batch 0/3750] [D loss: 0.13775229454040527] [G loss: -1.0043838024139404]\n",
      "[Epoch 2/10] [Batch 100/3750] [D loss: 0.039384979754686356] [G loss: 0.11758723855018616]\n",
      "[Epoch 2/10] [Batch 200/3750] [D loss: 0.014727175235748291] [G loss: 0.26028308272361755]\n",
      "[Epoch 2/10] [Batch 300/3750] [D loss: -0.3193046450614929] [G loss: -0.7803259491920471]\n",
      "[Epoch 2/10] [Batch 400/3750] [D loss: -0.09536623954772949] [G loss: 1.244988203048706]\n",
      "[Epoch 2/10] [Batch 500/3750] [D loss: -0.30644434690475464] [G loss: -0.0918947085738182]\n",
      "[Epoch 2/10] [Batch 600/3750] [D loss: -0.06991438567638397] [G loss: 0.5722230672836304]\n",
      "[Epoch 2/10] [Batch 700/3750] [D loss: -0.1211991161108017] [G loss: 0.19723880290985107]\n",
      "[Epoch 2/10] [Batch 800/3750] [D loss: -0.1001218855381012] [G loss: -0.1171303242444992]\n",
      "[Epoch 2/10] [Batch 900/3750] [D loss: -0.17715851962566376] [G loss: 0.3338019847869873]\n",
      "[Epoch 2/10] [Batch 1000/3750] [D loss: -0.19220800697803497] [G loss: -0.009293332695960999]\n",
      "[Epoch 2/10] [Batch 1100/3750] [D loss: -0.09741954505443573] [G loss: 0.21252983808517456]\n",
      "[Epoch 2/10] [Batch 1200/3750] [D loss: -0.34506481885910034] [G loss: -0.661066472530365]\n",
      "[Epoch 2/10] [Batch 1300/3750] [D loss: -0.11714635044336319] [G loss: -0.1118798702955246]\n",
      "[Epoch 2/10] [Batch 1400/3750] [D loss: -0.040966570377349854] [G loss: 0.37514933943748474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/10] [Batch 1500/3750] [D loss: 0.06079000234603882] [G loss: 0.5674723386764526]\n",
      "[Epoch 2/10] [Batch 1600/3750] [D loss: -0.059599995613098145] [G loss: 0.5073263645172119]\n",
      "[Epoch 2/10] [Batch 1700/3750] [D loss: -0.028300940990447998] [G loss: -0.1871200054883957]\n",
      "[Epoch 2/10] [Batch 1800/3750] [D loss: -0.053586721420288086] [G loss: -2.125556230545044]\n",
      "[Epoch 2/10] [Batch 1900/3750] [D loss: -0.09528446197509766] [G loss: -0.35791313648223877]\n",
      "[Epoch 2/10] [Batch 2000/3750] [D loss: -0.010291635990142822] [G loss: 0.6099671125411987]\n",
      "[Epoch 2/10] [Batch 2100/3750] [D loss: -0.23707059025764465] [G loss: 0.7361066341400146]\n",
      "[Epoch 2/10] [Batch 2200/3750] [D loss: -0.14030885696411133] [G loss: -0.9493051767349243]\n",
      "[Epoch 2/10] [Batch 2300/3750] [D loss: -0.12369069457054138] [G loss: -0.3546847403049469]\n",
      "[Epoch 2/10] [Batch 2400/3750] [D loss: -0.2580986022949219] [G loss: -0.43570512533187866]\n",
      "[Epoch 2/10] [Batch 2500/3750] [D loss: -0.20661848783493042] [G loss: -0.735874593257904]\n",
      "[Epoch 2/10] [Batch 2600/3750] [D loss: -0.2409336417913437] [G loss: 0.3825809955596924]\n",
      "[Epoch 2/10] [Batch 2700/3750] [D loss: -0.20044386386871338] [G loss: -0.2199930101633072]\n",
      "[Epoch 2/10] [Batch 2800/3750] [D loss: -0.23406654596328735] [G loss: 0.051113106310367584]\n",
      "[Epoch 2/10] [Batch 2900/3750] [D loss: -0.07400453835725784] [G loss: -0.034839875996112823]\n",
      "[Epoch 2/10] [Batch 3000/3750] [D loss: -0.19429679214954376] [G loss: 0.3632577955722809]\n",
      "[Epoch 2/10] [Batch 3100/3750] [D loss: -0.14628735184669495] [G loss: -0.015501650050282478]\n",
      "[Epoch 2/10] [Batch 3200/3750] [D loss: -0.23391377925872803] [G loss: -0.7463386058807373]\n",
      "[Epoch 2/10] [Batch 3300/3750] [D loss: -0.25243085622787476] [G loss: 0.577980101108551]\n",
      "[Epoch 2/10] [Batch 3400/3750] [D loss: -0.1978881061077118] [G loss: 0.5060470104217529]\n",
      "[Epoch 2/10] [Batch 3500/3750] [D loss: 0.04897996783256531] [G loss: 0.5479817390441895]\n",
      "[Epoch 2/10] [Batch 3600/3750] [D loss: -0.16767829656600952] [G loss: 0.4502089023590088]\n",
      "[Epoch 2/10] [Batch 3700/3750] [D loss: -0.14256536960601807] [G loss: 0.40376341342926025]\n",
      "[Epoch 3/10] [Batch 0/3750] [D loss: -0.1859525442123413] [G loss: 0.3522803485393524]\n",
      "[Epoch 3/10] [Batch 100/3750] [D loss: -0.13617748022079468] [G loss: -0.5649285912513733]\n",
      "[Epoch 3/10] [Batch 200/3750] [D loss: -0.05612444877624512] [G loss: -0.4318242371082306]\n",
      "[Epoch 3/10] [Batch 300/3750] [D loss: -0.13340452313423157] [G loss: -0.22784022986888885]\n",
      "[Epoch 3/10] [Batch 400/3750] [D loss: -0.3168754577636719] [G loss: 0.19092720746994019]\n",
      "[Epoch 3/10] [Batch 500/3750] [D loss: -0.2179436981678009] [G loss: -0.36615538597106934]\n",
      "[Epoch 3/10] [Batch 600/3750] [D loss: -0.10128063708543777] [G loss: 0.17785513401031494]\n",
      "[Epoch 3/10] [Batch 700/3750] [D loss: -0.16759085655212402] [G loss: 0.252752423286438]\n",
      "[Epoch 3/10] [Batch 800/3750] [D loss: 0.2743176221847534] [G loss: 0.6694684028625488]\n",
      "[Epoch 3/10] [Batch 900/3750] [D loss: -0.18457871675491333] [G loss: 0.03927361220121384]\n",
      "[Epoch 3/10] [Batch 1000/3750] [D loss: -0.18725115060806274] [G loss: -0.21053917706012726]\n",
      "[Epoch 3/10] [Batch 1100/3750] [D loss: -0.21006500720977783] [G loss: 0.2511930465698242]\n",
      "[Epoch 3/10] [Batch 1200/3750] [D loss: -0.04006657004356384] [G loss: 0.3717319965362549]\n",
      "[Epoch 3/10] [Batch 1300/3750] [D loss: -0.023326992988586426] [G loss: -0.3699755072593689]\n",
      "[Epoch 3/10] [Batch 1400/3750] [D loss: -0.053476423025131226] [G loss: -0.09944657236337662]\n",
      "[Epoch 3/10] [Batch 1500/3750] [D loss: -0.17375455796718597] [G loss: 0.2525957226753235]\n",
      "[Epoch 3/10] [Batch 1600/3750] [D loss: -0.16992974281311035] [G loss: -0.8803820610046387]\n",
      "[Epoch 3/10] [Batch 1700/3750] [D loss: -0.12186053395271301] [G loss: -0.20445522665977478]\n",
      "[Epoch 3/10] [Batch 1800/3750] [D loss: 0.17415356636047363] [G loss: -0.4035707712173462]\n",
      "[Epoch 3/10] [Batch 1900/3750] [D loss: -0.14856219291687012] [G loss: -0.6261537075042725]\n",
      "[Epoch 3/10] [Batch 2000/3750] [D loss: -0.19312673807144165] [G loss: 0.6394009590148926]\n",
      "[Epoch 3/10] [Batch 2100/3750] [D loss: 0.022878319025039673] [G loss: 0.1932128518819809]\n",
      "[Epoch 3/10] [Batch 2200/3750] [D loss: -0.16360080242156982] [G loss: 0.4334029257297516]\n",
      "[Epoch 3/10] [Batch 2300/3750] [D loss: 0.20002073049545288] [G loss: -0.457987517118454]\n",
      "[Epoch 3/10] [Batch 2400/3750] [D loss: 0.0645110011100769] [G loss: 0.49370479583740234]\n",
      "[Epoch 3/10] [Batch 2500/3750] [D loss: -0.05386999249458313] [G loss: -0.16902270913124084]\n",
      "[Epoch 3/10] [Batch 2600/3750] [D loss: 0.024462729692459106] [G loss: -0.21647799015045166]\n",
      "[Epoch 3/10] [Batch 2700/3750] [D loss: -0.009989157319068909] [G loss: -0.23258492350578308]\n",
      "[Epoch 3/10] [Batch 2800/3750] [D loss: -0.27597615122795105] [G loss: 0.7974117994308472]\n",
      "[Epoch 3/10] [Batch 2900/3750] [D loss: 0.07948894053697586] [G loss: -0.15230689942836761]\n",
      "[Epoch 3/10] [Batch 3000/3750] [D loss: -0.10596275329589844] [G loss: -0.003788866102695465]\n",
      "[Epoch 3/10] [Batch 3100/3750] [D loss: 0.004729747772216797] [G loss: -0.5965876579284668]\n",
      "[Epoch 3/10] [Batch 3200/3750] [D loss: -0.06807154417037964] [G loss: 0.45011085271835327]\n",
      "[Epoch 3/10] [Batch 3300/3750] [D loss: -0.14162206649780273] [G loss: 0.4947609305381775]\n",
      "[Epoch 3/10] [Batch 3400/3750] [D loss: -0.04935981333255768] [G loss: -0.04794391617178917]\n",
      "[Epoch 3/10] [Batch 3500/3750] [D loss: -0.0880826860666275] [G loss: 0.07461672276258469]\n",
      "[Epoch 3/10] [Batch 3600/3750] [D loss: -0.024687349796295166] [G loss: -0.36121007800102234]\n",
      "[Epoch 3/10] [Batch 3700/3750] [D loss: -0.014059126377105713] [G loss: -0.24093523621559143]\n",
      "[Epoch 4/10] [Batch 0/3750] [D loss: -0.029300644993782043] [G loss: 0.05742013454437256]\n",
      "[Epoch 4/10] [Batch 100/3750] [D loss: -0.26952850818634033] [G loss: 0.8424413800239563]\n",
      "[Epoch 4/10] [Batch 200/3750] [D loss: 0.1864159107208252] [G loss: -0.5148376226425171]\n",
      "[Epoch 4/10] [Batch 300/3750] [D loss: -0.040499866008758545] [G loss: -0.3533867597579956]\n",
      "[Epoch 4/10] [Batch 400/3750] [D loss: -0.15281695127487183] [G loss: 0.6830049753189087]\n",
      "[Epoch 4/10] [Batch 500/3750] [D loss: -0.04146399348974228] [G loss: 0.07972148805856705]\n",
      "[Epoch 4/10] [Batch 600/3750] [D loss: 0.012177616357803345] [G loss: -0.39490577578544617]\n",
      "[Epoch 4/10] [Batch 700/3750] [D loss: -0.14530599117279053] [G loss: -0.11228471249341965]\n",
      "[Epoch 4/10] [Batch 800/3750] [D loss: -0.07885841280221939] [G loss: 0.09141196310520172]\n",
      "[Epoch 4/10] [Batch 900/3750] [D loss: 0.0507623553276062] [G loss: 0.16937415301799774]\n",
      "[Epoch 4/10] [Batch 1000/3750] [D loss: -0.08924544602632523] [G loss: 0.035722509026527405]\n",
      "[Epoch 4/10] [Batch 1100/3750] [D loss: -0.22211983799934387] [G loss: 0.03314446657896042]\n",
      "[Epoch 4/10] [Batch 1200/3750] [D loss: -0.5517013072967529] [G loss: 1.1527620553970337]\n",
      "[Epoch 4/10] [Batch 1300/3750] [D loss: 0.07147655636072159] [G loss: 0.2965623736381531]\n",
      "[Epoch 4/10] [Batch 1400/3750] [D loss: 0.08658484369516373] [G loss: 0.1425529420375824]\n",
      "[Epoch 4/10] [Batch 1500/3750] [D loss: 0.0018445849418640137] [G loss: 0.09923676401376724]\n",
      "[Epoch 4/10] [Batch 1600/3750] [D loss: -0.001092374324798584] [G loss: -0.13059324026107788]\n",
      "[Epoch 4/10] [Batch 1700/3750] [D loss: -0.04963487386703491] [G loss: -0.08057066053152084]\n",
      "[Epoch 4/10] [Batch 1800/3750] [D loss: -0.05280211567878723] [G loss: 0.37772223353385925]\n",
      "[Epoch 4/10] [Batch 1900/3750] [D loss: -0.065976582467556] [G loss: -0.2495313137769699]\n",
      "[Epoch 4/10] [Batch 2000/3750] [D loss: -0.008904695510864258] [G loss: -0.14973799884319305]\n",
      "[Epoch 4/10] [Batch 2100/3750] [D loss: -0.010372266173362732] [G loss: 0.21500563621520996]\n",
      "[Epoch 4/10] [Batch 2200/3750] [D loss: -0.03947645425796509] [G loss: 0.14644844830036163]\n",
      "[Epoch 4/10] [Batch 2300/3750] [D loss: -0.03958551585674286] [G loss: 0.14521950483322144]\n",
      "[Epoch 4/10] [Batch 2400/3750] [D loss: -0.19849973917007446] [G loss: 0.8079608678817749]\n",
      "[Epoch 4/10] [Batch 2500/3750] [D loss: -0.13177178800106049] [G loss: 0.1042683869600296]\n",
      "[Epoch 4/10] [Batch 2600/3750] [D loss: 0.056638479232788086] [G loss: -0.4394338130950928]\n",
      "[Epoch 4/10] [Batch 2700/3750] [D loss: -0.1505601704120636] [G loss: 0.5933107137680054]\n",
      "[Epoch 4/10] [Batch 2800/3750] [D loss: -0.13043761253356934] [G loss: -0.470327228307724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/10] [Batch 2900/3750] [D loss: 0.017080307006835938] [G loss: 0.015188068151473999]\n",
      "[Epoch 4/10] [Batch 3000/3750] [D loss: -0.07230830192565918] [G loss: 1.3973942995071411]\n",
      "[Epoch 4/10] [Batch 3100/3750] [D loss: -0.5918034315109253] [G loss: 2.2721548080444336]\n",
      "[Epoch 4/10] [Batch 3200/3750] [D loss: -0.026296377182006836] [G loss: -0.3170163929462433]\n",
      "[Epoch 4/10] [Batch 3300/3750] [D loss: -0.20544064044952393] [G loss: 0.15789783000946045]\n",
      "[Epoch 4/10] [Batch 3400/3750] [D loss: -0.004519820213317871] [G loss: 0.296251505613327]\n",
      "[Epoch 4/10] [Batch 3500/3750] [D loss: -0.024879667907953262] [G loss: -0.007042234297841787]\n",
      "[Epoch 4/10] [Batch 3600/3750] [D loss: 0.1984536349773407] [G loss: -0.25843510031700134]\n",
      "[Epoch 4/10] [Batch 3700/3750] [D loss: -0.00458449125289917] [G loss: -0.43060046434402466]\n",
      "[Epoch 5/10] [Batch 0/3750] [D loss: -0.07321912050247192] [G loss: -0.32612594962120056]\n",
      "[Epoch 5/10] [Batch 100/3750] [D loss: -0.02584215998649597] [G loss: -0.206874281167984]\n",
      "[Epoch 5/10] [Batch 200/3750] [D loss: 0.03892022371292114] [G loss: -0.2424483299255371]\n",
      "[Epoch 5/10] [Batch 300/3750] [D loss: -0.1691071093082428] [G loss: -0.3506412208080292]\n",
      "[Epoch 5/10] [Batch 400/3750] [D loss: -0.23539894819259644] [G loss: 1.0812792778015137]\n",
      "[Epoch 5/10] [Batch 500/3750] [D loss: -0.40010058879852295] [G loss: 1.0406893491744995]\n",
      "[Epoch 5/10] [Batch 600/3750] [D loss: 0.051880836486816406] [G loss: 0.18149042129516602]\n",
      "[Epoch 5/10] [Batch 700/3750] [D loss: 0.10704274475574493] [G loss: -0.1888970285654068]\n",
      "[Epoch 5/10] [Batch 800/3750] [D loss: -0.030582278966903687] [G loss: -0.2678103744983673]\n",
      "[Epoch 5/10] [Batch 900/3750] [D loss: -0.23762105405330658] [G loss: -0.26739591360092163]\n",
      "[Epoch 5/10] [Batch 1000/3750] [D loss: -0.18635272979736328] [G loss: 0.5797808170318604]\n",
      "[Epoch 5/10] [Batch 1100/3750] [D loss: 0.07689955085515976] [G loss: -0.03708351403474808]\n",
      "[Epoch 5/10] [Batch 1200/3750] [D loss: -0.04646387696266174] [G loss: -0.1517302691936493]\n",
      "[Epoch 5/10] [Batch 1300/3750] [D loss: 0.09795747697353363] [G loss: 0.05319181829690933]\n",
      "[Epoch 5/10] [Batch 1400/3750] [D loss: -0.004886150360107422] [G loss: 0.20671537518501282]\n",
      "[Epoch 5/10] [Batch 1500/3750] [D loss: -0.2448614090681076] [G loss: 0.21191617846488953]\n",
      "[Epoch 5/10] [Batch 1600/3750] [D loss: -0.1304616630077362] [G loss: 0.6060686111450195]\n",
      "[Epoch 5/10] [Batch 1700/3750] [D loss: -0.005436688661575317] [G loss: 0.15258550643920898]\n",
      "[Epoch 5/10] [Batch 1800/3750] [D loss: -0.03325923532247543] [G loss: 0.013195588253438473]\n",
      "[Epoch 5/10] [Batch 1900/3750] [D loss: -0.0029809102416038513] [G loss: -0.0013838950544595718]\n",
      "[Epoch 5/10] [Batch 2000/3750] [D loss: -0.0714152455329895] [G loss: 0.4458077549934387]\n",
      "[Epoch 5/10] [Batch 2100/3750] [D loss: -0.08998097479343414] [G loss: -0.14615127444267273]\n",
      "[Epoch 5/10] [Batch 2200/3750] [D loss: -0.01160457730293274] [G loss: -0.0006748586893081665]\n",
      "[Epoch 5/10] [Batch 2300/3750] [D loss: 0.08769881725311279] [G loss: -0.011387134902179241]\n",
      "[Epoch 5/10] [Batch 2400/3750] [D loss: -0.31033584475517273] [G loss: 0.4665493369102478]\n",
      "[Epoch 5/10] [Batch 2500/3750] [D loss: -0.28889188170433044] [G loss: 0.8973108530044556]\n",
      "[Epoch 5/10] [Batch 2600/3750] [D loss: 0.010735541582107544] [G loss: 0.003874670248478651]\n",
      "[Epoch 5/10] [Batch 2700/3750] [D loss: 0.06321302801370621] [G loss: 0.09319472312927246]\n",
      "[Epoch 5/10] [Batch 2800/3750] [D loss: 0.16070079803466797] [G loss: -2.226313591003418]\n",
      "[Epoch 5/10] [Batch 2900/3750] [D loss: 0.06574368476867676] [G loss: 0.6997215747833252]\n",
      "[Epoch 5/10] [Batch 3000/3750] [D loss: -0.031535834074020386] [G loss: 0.0795956403017044]\n",
      "[Epoch 5/10] [Batch 3100/3750] [D loss: -0.012732978910207748] [G loss: -0.02495419606566429]\n",
      "[Epoch 5/10] [Batch 3200/3750] [D loss: -0.023343980312347412] [G loss: -0.12269983440637589]\n",
      "[Epoch 5/10] [Batch 3300/3750] [D loss: -0.06330527365207672] [G loss: -0.02408541552722454]\n",
      "[Epoch 5/10] [Batch 3400/3750] [D loss: -0.4650076627731323] [G loss: -0.45290908217430115]\n",
      "[Epoch 5/10] [Batch 3500/3750] [D loss: -0.06633670628070831] [G loss: -0.13363635540008545]\n",
      "[Epoch 5/10] [Batch 3600/3750] [D loss: -0.03145524859428406] [G loss: -0.29177796840667725]\n",
      "[Epoch 5/10] [Batch 3700/3750] [D loss: -0.253467857837677] [G loss: -0.9648618102073669]\n",
      "[Epoch 6/10] [Batch 0/3750] [D loss: -0.07580715417861938] [G loss: -0.3347010612487793]\n",
      "[Epoch 6/10] [Batch 100/3750] [D loss: -0.10468387603759766] [G loss: 1.1399970054626465]\n",
      "[Epoch 6/10] [Batch 200/3750] [D loss: -0.0525670051574707] [G loss: -0.6429448127746582]\n",
      "[Epoch 6/10] [Batch 300/3750] [D loss: 0.08294951915740967] [G loss: 0.3722509443759918]\n",
      "[Epoch 6/10] [Batch 400/3750] [D loss: -0.02192835509777069] [G loss: 0.2203640639781952]\n",
      "[Epoch 6/10] [Batch 500/3750] [D loss: -0.09197890013456345] [G loss: 0.3592976927757263]\n",
      "[Epoch 6/10] [Batch 600/3750] [D loss: 0.16931411623954773] [G loss: 0.44520023465156555]\n",
      "[Epoch 6/10] [Batch 700/3750] [D loss: -0.1243698000907898] [G loss: 0.1985664963722229]\n",
      "[Epoch 6/10] [Batch 800/3750] [D loss: 0.021858394145965576] [G loss: -0.36146795749664307]\n",
      "[Epoch 6/10] [Batch 900/3750] [D loss: 0.03785121440887451] [G loss: -0.5551481246948242]\n",
      "[Epoch 6/10] [Batch 1000/3750] [D loss: -0.05864851176738739] [G loss: -0.10605385154485703]\n",
      "[Epoch 6/10] [Batch 1100/3750] [D loss: 0.015615880489349365] [G loss: 0.4129907786846161]\n",
      "[Epoch 6/10] [Batch 1200/3750] [D loss: -0.0006853342056274414] [G loss: 0.6569177508354187]\n",
      "[Epoch 6/10] [Batch 1300/3750] [D loss: 0.1596183180809021] [G loss: -0.22886860370635986]\n",
      "[Epoch 6/10] [Batch 1400/3750] [D loss: -0.018515050411224365] [G loss: 0.2388804852962494]\n",
      "[Epoch 6/10] [Batch 1500/3750] [D loss: -0.0002491399645805359] [G loss: 0.2593655288219452]\n",
      "[Epoch 6/10] [Batch 1600/3750] [D loss: -0.032147638499736786] [G loss: -0.013128072954714298]\n",
      "[Epoch 6/10] [Batch 1700/3750] [D loss: -0.004713833332061768] [G loss: -0.8888452053070068]\n",
      "[Epoch 6/10] [Batch 1800/3750] [D loss: -0.0492856502532959] [G loss: -0.8168185353279114]\n",
      "[Epoch 6/10] [Batch 1900/3750] [D loss: -0.07333654165267944] [G loss: -0.18678325414657593]\n",
      "[Epoch 6/10] [Batch 2000/3750] [D loss: -0.015579797327518463] [G loss: 0.14266926050186157]\n",
      "[Epoch 6/10] [Batch 2100/3750] [D loss: 0.003932967782020569] [G loss: 0.23871061205863953]\n",
      "[Epoch 6/10] [Batch 2200/3750] [D loss: -0.03420364856719971] [G loss: 0.09341250360012054]\n",
      "[Epoch 6/10] [Batch 2300/3750] [D loss: -0.7031707763671875] [G loss: 2.2221128940582275]\n",
      "[Epoch 6/10] [Batch 2400/3750] [D loss: -0.2243650257587433] [G loss: -0.45720118284225464]\n",
      "[Epoch 6/10] [Batch 2500/3750] [D loss: 0.1620551347732544] [G loss: 0.896209716796875]\n",
      "[Epoch 6/10] [Batch 2600/3750] [D loss: -0.1514078974723816] [G loss: -0.3075593411922455]\n",
      "[Epoch 6/10] [Batch 2700/3750] [D loss: 0.012283504009246826] [G loss: 0.35556578636169434]\n",
      "[Epoch 6/10] [Batch 2800/3750] [D loss: -0.02512364089488983] [G loss: 0.0939594954252243]\n",
      "[Epoch 6/10] [Batch 2900/3750] [D loss: 0.06206473708152771] [G loss: 0.12683144211769104]\n",
      "[Epoch 6/10] [Batch 3000/3750] [D loss: -0.27084648609161377] [G loss: 0.7246798276901245]\n",
      "[Epoch 6/10] [Batch 3100/3750] [D loss: 0.03866102546453476] [G loss: 0.008016543462872505]\n",
      "[Epoch 6/10] [Batch 3200/3750] [D loss: -0.13995684683322906] [G loss: -0.10771996527910233]\n",
      "[Epoch 6/10] [Batch 3300/3750] [D loss: -0.14717167615890503] [G loss: -0.6842911243438721]\n",
      "[Epoch 6/10] [Batch 3400/3750] [D loss: -0.12495550513267517] [G loss: 0.45145028829574585]\n",
      "[Epoch 6/10] [Batch 3500/3750] [D loss: -0.029262572526931763] [G loss: 0.24478206038475037]\n",
      "[Epoch 6/10] [Batch 3600/3750] [D loss: -0.011421412229537964] [G loss: 0.16965878009796143]\n",
      "[Epoch 6/10] [Batch 3700/3750] [D loss: -0.03697460889816284] [G loss: 0.6679447293281555]\n",
      "[Epoch 7/10] [Batch 0/3750] [D loss: -0.09307960420846939] [G loss: -0.11657297611236572]\n",
      "[Epoch 7/10] [Batch 100/3750] [D loss: -0.13049404323101044] [G loss: 0.459953635931015]\n",
      "[Epoch 7/10] [Batch 200/3750] [D loss: -0.013268291018903255] [G loss: -0.0713176354765892]\n",
      "[Epoch 7/10] [Batch 300/3750] [D loss: -0.027093034237623215] [G loss: -0.04154372215270996]\n",
      "[Epoch 7/10] [Batch 400/3750] [D loss: -0.032048553228378296] [G loss: -0.2171088457107544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/10] [Batch 500/3750] [D loss: -0.05206114053726196] [G loss: 0.24738964438438416]\n",
      "[Epoch 7/10] [Batch 600/3750] [D loss: 0.012649953365325928] [G loss: -0.4579246938228607]\n",
      "[Epoch 7/10] [Batch 700/3750] [D loss: 0.2541317939758301] [G loss: 1.3889122009277344]\n",
      "[Epoch 7/10] [Batch 800/3750] [D loss: 0.17884927988052368] [G loss: 0.6247106790542603]\n",
      "[Epoch 7/10] [Batch 900/3750] [D loss: -0.037195056676864624] [G loss: 0.16082951426506042]\n",
      "[Epoch 7/10] [Batch 1000/3750] [D loss: -0.742232084274292] [G loss: 2.438591480255127]\n",
      "[Epoch 7/10] [Batch 1100/3750] [D loss: 0.014707803726196289] [G loss: 0.2458633929491043]\n",
      "[Epoch 7/10] [Batch 1200/3750] [D loss: -0.6909189224243164] [G loss: -1.4432116746902466]\n",
      "[Epoch 7/10] [Batch 1300/3750] [D loss: 0.006399750709533691] [G loss: 0.020867131650447845]\n",
      "[Epoch 7/10] [Batch 1400/3750] [D loss: 0.078066386282444] [G loss: -0.0028348376508802176]\n",
      "[Epoch 7/10] [Batch 1500/3750] [D loss: -0.013565748929977417] [G loss: 0.12783363461494446]\n",
      "[Epoch 7/10] [Batch 1600/3750] [D loss: -0.08857965469360352] [G loss: 1.3721463680267334]\n",
      "[Epoch 7/10] [Batch 1700/3750] [D loss: -0.03244900703430176] [G loss: -0.048476025462150574]\n",
      "[Epoch 7/10] [Batch 1800/3750] [D loss: 0.009369993582367897] [G loss: -0.022568199783563614]\n",
      "[Epoch 7/10] [Batch 1900/3750] [D loss: -0.033130764961242676] [G loss: -0.21442720293998718]\n",
      "[Epoch 7/10] [Batch 2000/3750] [D loss: -0.01798296719789505] [G loss: 0.10959797352552414]\n",
      "[Epoch 7/10] [Batch 2100/3750] [D loss: -0.07053659111261368] [G loss: -0.05985638126730919]\n",
      "[Epoch 7/10] [Batch 2200/3750] [D loss: 0.05306136608123779] [G loss: 0.36299222707748413]\n",
      "[Epoch 7/10] [Batch 2300/3750] [D loss: 0.013386368751525879] [G loss: -0.7360051870346069]\n",
      "[Epoch 7/10] [Batch 2400/3750] [D loss: 0.1948946714401245] [G loss: 0.5942937135696411]\n",
      "[Epoch 7/10] [Batch 2500/3750] [D loss: -0.03503426909446716] [G loss: -0.18658991158008575]\n",
      "[Epoch 7/10] [Batch 2600/3750] [D loss: -0.009201876819133759] [G loss: 0.03212611749768257]\n",
      "[Epoch 7/10] [Batch 2700/3750] [D loss: -0.025100477039813995] [G loss: -0.06229468807578087]\n",
      "[Epoch 7/10] [Batch 2800/3750] [D loss: -0.03787302225828171] [G loss: -0.06012050434947014]\n",
      "[Epoch 7/10] [Batch 2900/3750] [D loss: -0.009706398472189903] [G loss: 0.11608675867319107]\n",
      "[Epoch 7/10] [Batch 3000/3750] [D loss: 0.02143844962120056] [G loss: -0.15737785398960114]\n",
      "[Epoch 7/10] [Batch 3100/3750] [D loss: -0.1674186736345291] [G loss: -0.1978967785835266]\n",
      "[Epoch 7/10] [Batch 3200/3750] [D loss: -0.03521573543548584] [G loss: 0.47467172145843506]\n",
      "[Epoch 7/10] [Batch 3300/3750] [D loss: -0.016490668058395386] [G loss: -0.42640596628189087]\n",
      "[Epoch 7/10] [Batch 3400/3750] [D loss: 0.05917920172214508] [G loss: 0.11857713013887405]\n",
      "[Epoch 7/10] [Batch 3500/3750] [D loss: -0.18359678983688354] [G loss: -0.4767917990684509]\n",
      "[Epoch 7/10] [Batch 3600/3750] [D loss: -0.009772300720214844] [G loss: -0.7355794310569763]\n",
      "[Epoch 7/10] [Batch 3700/3750] [D loss: 0.09673091769218445] [G loss: -0.28124743700027466]\n",
      "[Epoch 8/10] [Batch 0/3750] [D loss: 0.08308535814285278] [G loss: 0.013425244018435478]\n",
      "[Epoch 8/10] [Batch 100/3750] [D loss: 0.013729557394981384] [G loss: 0.022009072825312614]\n",
      "[Epoch 8/10] [Batch 200/3750] [D loss: 0.026319295167922974] [G loss: -0.08874478936195374]\n",
      "[Epoch 8/10] [Batch 300/3750] [D loss: -0.012774020433425903] [G loss: 0.2017160952091217]\n",
      "[Epoch 8/10] [Batch 400/3750] [D loss: -0.0016633588820695877] [G loss: -0.09159094840288162]\n",
      "[Epoch 8/10] [Batch 500/3750] [D loss: -0.1549014151096344] [G loss: -0.4199153184890747]\n",
      "[Epoch 8/10] [Batch 600/3750] [D loss: -0.11358848214149475] [G loss: -0.335038959980011]\n",
      "[Epoch 8/10] [Batch 700/3750] [D loss: -0.24631240963935852] [G loss: -0.32872384786605835]\n",
      "[Epoch 8/10] [Batch 800/3750] [D loss: 0.0015361011028289795] [G loss: -0.08648529648780823]\n",
      "[Epoch 8/10] [Batch 900/3750] [D loss: -0.26259279251098633] [G loss: 0.5496255159378052]\n",
      "[Epoch 8/10] [Batch 1000/3750] [D loss: 0.02247917652130127] [G loss: -0.16434261202812195]\n",
      "[Epoch 8/10] [Batch 1100/3750] [D loss: -0.08968770503997803] [G loss: 0.869036078453064]\n",
      "[Epoch 8/10] [Batch 1200/3750] [D loss: -0.012171387672424316] [G loss: -0.2964394986629486]\n",
      "[Epoch 8/10] [Batch 1300/3750] [D loss: 0.128698468208313] [G loss: -0.4936450719833374]\n",
      "[Epoch 8/10] [Batch 1400/3750] [D loss: -0.07443958520889282] [G loss: 0.4478558599948883]\n",
      "[Epoch 8/10] [Batch 1500/3750] [D loss: 0.03571516275405884] [G loss: 0.023346509784460068]\n",
      "[Epoch 8/10] [Batch 1600/3750] [D loss: -0.004839636385440826] [G loss: 0.07330812513828278]\n",
      "[Epoch 8/10] [Batch 1700/3750] [D loss: -0.004203319549560547] [G loss: 0.22356373071670532]\n",
      "[Epoch 8/10] [Batch 1800/3750] [D loss: -0.06834942102432251] [G loss: -0.18785783648490906]\n",
      "[Epoch 8/10] [Batch 1900/3750] [D loss: -0.058677852153778076] [G loss: 1.079974889755249]\n",
      "[Epoch 8/10] [Batch 2000/3750] [D loss: -0.13655993342399597] [G loss: 0.06063242256641388]\n",
      "[Epoch 8/10] [Batch 2100/3750] [D loss: -0.06561431288719177] [G loss: 0.2597872018814087]\n",
      "[Epoch 8/10] [Batch 2200/3750] [D loss: -0.027609027922153473] [G loss: 0.13238272070884705]\n",
      "[Epoch 8/10] [Batch 2300/3750] [D loss: 0.07674312591552734] [G loss: -0.3056018352508545]\n",
      "[Epoch 8/10] [Batch 2400/3750] [D loss: -0.11516308784484863] [G loss: 0.5210093855857849]\n",
      "[Epoch 8/10] [Batch 2500/3750] [D loss: -0.0618709921836853] [G loss: -0.32939112186431885]\n",
      "[Epoch 8/10] [Batch 2600/3750] [D loss: 0.034933559596538544] [G loss: 0.0557904914021492]\n",
      "[Epoch 8/10] [Batch 2700/3750] [D loss: 0.16146481037139893] [G loss: 0.21334408223628998]\n",
      "[Epoch 8/10] [Batch 2800/3750] [D loss: -0.0749174952507019] [G loss: 0.30916017293930054]\n",
      "[Epoch 8/10] [Batch 2900/3750] [D loss: 0.00802953913807869] [G loss: 0.02522517926990986]\n",
      "[Epoch 8/10] [Batch 3000/3750] [D loss: -0.2181699275970459] [G loss: 1.285304069519043]\n",
      "[Epoch 8/10] [Batch 3100/3750] [D loss: 0.08167538046836853] [G loss: 0.24126175045967102]\n",
      "[Epoch 8/10] [Batch 3200/3750] [D loss: -0.29378455877304077] [G loss: 0.8311907052993774]\n",
      "[Epoch 8/10] [Batch 3300/3750] [D loss: 0.17178525030612946] [G loss: -0.24063661694526672]\n",
      "[Epoch 8/10] [Batch 3400/3750] [D loss: 0.0056321946904063225] [G loss: -0.002917530480772257]\n",
      "[Epoch 8/10] [Batch 3500/3750] [D loss: -0.015727903693914413] [G loss: -0.020752523094415665]\n",
      "[Epoch 8/10] [Batch 3600/3750] [D loss: -0.013014256954193115] [G loss: -0.05508553981781006]\n",
      "[Epoch 8/10] [Batch 3700/3750] [D loss: -0.035043321549892426] [G loss: -0.01788186840713024]\n",
      "[Epoch 9/10] [Batch 0/3750] [D loss: 0.058091901242733] [G loss: 0.06439722329378128]\n",
      "[Epoch 9/10] [Batch 100/3750] [D loss: 0.3136332035064697] [G loss: -1.0618704557418823]\n",
      "[Epoch 9/10] [Batch 200/3750] [D loss: -0.055457934737205505] [G loss: 0.27043771743774414]\n",
      "[Epoch 9/10] [Batch 300/3750] [D loss: 0.06641930341720581] [G loss: -0.5950576066970825]\n",
      "[Epoch 9/10] [Batch 400/3750] [D loss: 0.0554543137550354] [G loss: -0.37749430537223816]\n",
      "[Epoch 9/10] [Batch 500/3750] [D loss: -0.07240462303161621] [G loss: -1.0469565391540527]\n",
      "[Epoch 9/10] [Batch 600/3750] [D loss: 0.08175456523895264] [G loss: -0.41600871086120605]\n",
      "[Epoch 9/10] [Batch 700/3750] [D loss: -0.020482227206230164] [G loss: -0.10992258787155151]\n",
      "[Epoch 9/10] [Batch 800/3750] [D loss: -0.07303044199943542] [G loss: 0.32338500022888184]\n",
      "[Epoch 9/10] [Batch 900/3750] [D loss: -0.04805725812911987] [G loss: -0.47524023056030273]\n",
      "[Epoch 9/10] [Batch 1000/3750] [D loss: -0.13665106892585754] [G loss: 0.49132877588272095]\n",
      "[Epoch 9/10] [Batch 1100/3750] [D loss: 0.16439592838287354] [G loss: 0.49973925948143005]\n",
      "[Epoch 9/10] [Batch 1200/3750] [D loss: 0.13071370124816895] [G loss: -0.7194156050682068]\n",
      "[Epoch 9/10] [Batch 1300/3750] [D loss: 0.06003612279891968] [G loss: -0.2838050127029419]\n",
      "[Epoch 9/10] [Batch 1400/3750] [D loss: -0.026677168905735016] [G loss: -0.02335437200963497]\n",
      "[Epoch 9/10] [Batch 1500/3750] [D loss: -0.03771067410707474] [G loss: 0.00365297868847847]\n",
      "[Epoch 9/10] [Batch 1600/3750] [D loss: -0.08720500767230988] [G loss: -0.21746650338172913]\n",
      "[Epoch 9/10] [Batch 1700/3750] [D loss: -0.08164133131504059] [G loss: 0.07578794658184052]\n",
      "[Epoch 9/10] [Batch 1800/3750] [D loss: 0.028190476819872856] [G loss: 0.030682235956192017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/10] [Batch 1900/3750] [D loss: -0.1434633731842041] [G loss: 0.379936158657074]\n",
      "[Epoch 9/10] [Batch 2000/3750] [D loss: 0.022326409816741943] [G loss: -0.5591810941696167]\n",
      "[Epoch 9/10] [Batch 2100/3750] [D loss: -0.10522960126399994] [G loss: -0.05189116671681404]\n",
      "[Epoch 9/10] [Batch 2200/3750] [D loss: -0.2744654417037964] [G loss: -0.11260104179382324]\n",
      "[Epoch 9/10] [Batch 2300/3750] [D loss: -0.04838836193084717] [G loss: -0.8663029074668884]\n",
      "[Epoch 9/10] [Batch 2400/3750] [D loss: -0.05054086446762085] [G loss: -0.24177628755569458]\n",
      "[Epoch 9/10] [Batch 2500/3750] [D loss: -0.1795225739479065] [G loss: -0.8157445192337036]\n",
      "[Epoch 9/10] [Batch 2600/3750] [D loss: -0.029933661222457886] [G loss: 0.2857365906238556]\n",
      "[Epoch 9/10] [Batch 2700/3750] [D loss: 0.07682500779628754] [G loss: 0.0703272745013237]\n",
      "[Epoch 9/10] [Batch 2800/3750] [D loss: 0.06161035597324371] [G loss: 0.1399630457162857]\n",
      "[Epoch 9/10] [Batch 2900/3750] [D loss: -0.028930924832820892] [G loss: 0.013900459744036198]\n",
      "[Epoch 9/10] [Batch 3000/3750] [D loss: -0.13598152995109558] [G loss: -0.37932270765304565]\n",
      "[Epoch 9/10] [Batch 3100/3750] [D loss: -0.12702780961990356] [G loss: 0.5135030746459961]\n",
      "[Epoch 9/10] [Batch 3200/3750] [D loss: -0.0016583502292633057] [G loss: 0.4374920129776001]\n",
      "[Epoch 9/10] [Batch 3300/3750] [D loss: 0.18362976610660553] [G loss: -0.0019713016226887703]\n",
      "[Epoch 9/10] [Batch 3400/3750] [D loss: -0.01727674901485443] [G loss: -0.13099880516529083]\n",
      "[Epoch 9/10] [Batch 3500/3750] [D loss: -0.024297982454299927] [G loss: 0.243587926030159]\n",
      "[Epoch 9/10] [Batch 3600/3750] [D loss: -0.07368940860033035] [G loss: 0.17647016048431396]\n",
      "[Epoch 9/10] [Batch 3700/3750] [D loss: -0.18431901931762695] [G loss: 0.9567235708236694]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Определение устройства (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Конфигурация\n",
    "class Config:\n",
    "    batch_size = 16\n",
    "    noise_size = 50\n",
    "    num_epochs = 10\n",
    "    clip_value = 0.01  # Предел для клиппинга весов дискриминатора\n",
    "    learning_rate = 0.0002\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./fashion_mnist', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "# Определение Генератора\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(config.noise_size, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "# Определение Дискриминатора\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "# Инициализация моделей\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Оптимизаторы\n",
    "optimizer_G = optim.RMSprop(generator.parameters(), lr=config.learning_rate)\n",
    "optimizer_D = optim.RMSprop(discriminator.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Тренировка\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "n_critic = 5\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "\n",
    "        # Настоящие и поддельные изображения\n",
    "        real_imgs = Variable(imgs.type(Tensor)).to(device)\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], config.noise_size)))).to(device)\n",
    "\n",
    "        # Тренировка дискриминатора\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Генерация поддельных изображений\n",
    "        fake_imgs = generator(z)\n",
    "\n",
    "        d_loss_real = -torch.mean(discriminator(real_imgs))\n",
    "        d_loss_fake = torch.mean(discriminator(fake_imgs.detach()))  # Detach fake_imgs для предотвращения повторного использования в генераторе\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Клиппинг весов дискриминатора\n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-config.clip_value, config.clip_value)\n",
    "\n",
    "        # Тренировка генератора каждые n_critic итераций\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "            # Генерируем новые поддельные изображения для генератора\n",
    "            fake_imgs_gen = generator(z)\n",
    "            g_loss = -torch.mean(discriminator(fake_imgs_gen))\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # Вывод прогресса\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Epoch {epoch}/{config.num_epochs}] [Batch {i}/{len(train_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "# Визуализация сгенерированных изображений\n",
    "z = Variable(Tensor(np.random.normal(0, 1, (16, config.noise_size)))).to(device)\n",
    "gen_imgs = generator(z)\n",
    "gen_imgs = gen_imgs.view(gen_imgs.size(0), 28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(gen_imgs[i].detach().cpu().numpy(), cmap='gray')  # Перенос на CPU для визуализации\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHKYfEyADcTR"
   },
   "source": [
    "# Замените клиппинг весов на штраф градентов. (10 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rk6b16dMDcyN",
    "outputId": "d0bbd825-2f7d-4ee2-ec0b-c93c4141809a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Определение устройства (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Конфигурация\n",
    "class Config:\n",
    "    batch_size = 16\n",
    "    noise_size = 100\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.0002\n",
    "    lambda_gp = 10\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Загрузка данных\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./fashion_mnist', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "# Определение Генератора\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(config.noise_size, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "# Определение Дискриминатора\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "# Инициализация моделей\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Оптимизаторы\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=config.learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=config.learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Функция для вычисления штрафа градиента\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    interpolated = Variable(alpha * real_samples + (1 - alpha) * fake_samples, requires_grad=True)\n",
    "    d_interpolated = D(interpolated)\n",
    "    fake = Variable(torch.ones(d_interpolated.size(), device=real_samples.device), requires_grad=False)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Тренировка\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "n_critic = 5\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "\n",
    "        # Конфигурация входных данных\n",
    "        real_imgs = Variable(imgs.type(Tensor)).to(device)\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], config.noise_size)))).to(device)\n",
    "\n",
    "        # Генерация поддельных изображений\n",
    "        fake_imgs = generator(z)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Тренировка Дискриминатора\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Реальные изображения\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        # Поддельные изображения\n",
    "        fake_validity = discriminator(fake_imgs)\n",
    "        # Штраф градиента\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "        # Функция потерь дискриминатора\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + config.lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Тренировка Генератора\n",
    "        # -----------------\n",
    "\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Генерируем изображение\n",
    "            generated_imgs = generator(z)\n",
    "            # Функция потерь генератора\n",
    "            g_loss = -torch.mean(discriminator(generated_imgs))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # Вывод прогресса\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[Epoch {epoch}/{config.num_epochs}] [Batch {i}/{len(train_loader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "# Визуализация сгенерированных изображений\n",
    "z = Variable(Tensor(np.random.normal(0, 1, (16, config.noise_size)))).to(device)\n",
    "gen_imgs = generator(z)\n",
    "gen_imgs = gen_imgs.view(gen_imgs.size(0), 28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(gen_imgs[i].detach().cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NphfrXRPFsDz"
   },
   "source": [
    "# Добавьте лейблы в WGAN, тем самым решая задачу условной генерации. (30 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fMVc6XtYFsYh",
    "outputId": "64114b0e-3a73-45a5-f42a-cfce2ecc7e7b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Конфигурация\n",
    "class Config:\n",
    "    batch_size = 16\n",
    "    noise_size = 50\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.0002\n",
    "    num_classes = 10  # Количество классов в FashionMNIST\n",
    "    embed_size = 50   # Размерность вектора встраивания для лейблов\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Датасет и DataLoader с использованием PyTorch Lightning\n",
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir='./fashion_mnist', batch_size=config.batch_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.fashion_mnist_train = FashionMNIST(self.data_dir, train=True, download=True, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.fashion_mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "# Определение Генератора\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(config.num_classes, config.embed_size)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(config.noise_size + config.embed_size, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([z, c], 1)\n",
    "        img = self.model(x)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "# Определение Дискриминатора\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(config.num_classes, config.embed_size)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28 + config.embed_size, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        c = self.label_emb(labels)\n",
    "        x = torch.cat([img_flat, c], 1)\n",
    "        validity = self.model(x)\n",
    "        return validity\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "        self.automatic_optimization = False  # Включаем ручное управление оптимизациями\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        return self.generator(z, labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, labels = batch\n",
    "        z = torch.randn(imgs.shape[0], config.noise_size).type_as(imgs)\n",
    "\n",
    "        # Получаем оптимизаторы\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        # Обучение дискриминатора\n",
    "        real_loss = torch.mean(self.discriminator(imgs, labels))\n",
    "        fake_imgs = self(z, labels).detach()\n",
    "        fake_loss = torch.mean(self.discriminator(fake_imgs, labels))\n",
    "        d_loss = real_loss - fake_loss\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "        # Обучение генератора\n",
    "        fake_imgs = self(z, labels)\n",
    "        g_loss = -torch.mean(self.discriminator(fake_imgs, labels))\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "\n",
    "        self.log('d_loss', d_loss, prog_bar=True)\n",
    "        self.log('g_loss', g_loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = optim.Adam(self.generator.parameters(), lr=config.learning_rate)\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=config.learning_rate)\n",
    "        return opt_g, opt_d\n",
    "\n",
    "# Инициализация и запуск тренировки\n",
    "dm = FashionMNISTDataModule()\n",
    "model = GAN()\n",
    "trainer = pl.Trainer(max_epochs=config.num_epochs)\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# Визуализация сгенерированных изображений\n",
    "z = torch.randn(16, config.noise_size)\n",
    "labels = torch.randint(0, config.num_classes, (16,))\n",
    "gen_imgs = model.generator(z, labels)\n",
    "gen_imgs = gen_imgs.view(gen_imgs.size(0), 28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(gen_imgs[i].detach().cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHVpNsNqMXQY"
   },
   "source": [
    "# Попытка объедения 2 и 3 заданий вместе!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T3pwQ4C0HzpE",
    "outputId": "8f8d59b6-1b47-4908-b2e5-5fe882dc41eb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Конфигурация\n",
    "class Config:\n",
    "    batch_size = 16\n",
    "    noise_size = 100\n",
    "    num_epochs = 5\n",
    "    learning_rate = 0.0002\n",
    "    lambda_gp = 10\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Датасет и DataLoader с использованием PyTorch Lightning\n",
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir='./fashion_mnist', batch_size=config.batch_size):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.fashion_mnist_train = FashionMNIST(self.data_dir, train=True, download=True, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.fashion_mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "# Определение Генератора\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(config.noise_size, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 28*28),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img\n",
    "    \n",
    "# Определение Дискриминатора\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1))\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "    \n",
    "# LightningModule для GAN\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(GAN, self).__init__()\n",
    "        self.generator = Generator()\n",
    "        self.discriminator = Discriminator()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=real_samples.device)\n",
    "        alpha = alpha.expand_as(real_samples)\n",
    "        interpolated = Variable(alpha * real_samples + (1 - alpha) * fake_samples, requires_grad=True)\n",
    "        d_interpolated = self.discriminator(interpolated)\n",
    "        fake = Variable(torch.ones(d_interpolated.size(), device=real_samples.device), requires_grad=False)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "        return gradient_penalty\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs = batch[0]\n",
    "        z = torch.randn(imgs.shape[0], config.noise_size).type_as(imgs)\n",
    "\n",
    "        # Получаем оптимизаторы\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        # Обучение дискриминатора\n",
    "        real_imgs = imgs\n",
    "        fake_imgs = self.generator(z)\n",
    "        \n",
    "        real_validity = self.discriminator(real_imgs)\n",
    "        fake_validity = self.discriminator(fake_imgs.detach())\n",
    "        gradient_penalty = self.compute_gradient_penalty(real_imgs, fake_imgs.detach())\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + config.lambda_gp * gradient_penalty\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "        # Обучение генератора\n",
    "        if batch_idx % 5 == 0:\n",
    "            generated_imgs = self.generator(z)\n",
    "            g_loss = -torch.mean(self.discriminator(generated_imgs))\n",
    "\n",
    "            opt_g.zero_grad()\n",
    "            self.manual_backward(g_loss)\n",
    "            opt_g.step()\n",
    "\n",
    "            self.log('g_loss', g_loss, prog_bar=True)\n",
    "\n",
    "        self.log('d_loss', d_loss, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = optim.Adam(self.generator.parameters(), lr=config.learning_rate, betas=(0.5, 0.999))\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=config.learning_rate, betas=(0.5, 0.999))\n",
    "        return opt_g, opt_d\n",
    "    \n",
    "# Инициализация и запуск тренировки\n",
    "dm = FashionMNISTDataModule()\n",
    "model = GAN()\n",
    "trainer = pl.Trainer(max_epochs=config.num_epochs)\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "# Визуализация сгенерированных изображений\n",
    "z = torch.randn(16, config.noise_size)\n",
    "gen_imgs = model.generator(z)\n",
    "gen_imgs = gen_imgs.view(gen_imgs.size(0), 28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(gen_imgs[i].detach().cpu().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
