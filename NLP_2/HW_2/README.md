# Retrieval-based chatbot Rick
![Rick](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/36d11c49-e839-4163-9e5f-5089833e42e9)

# Сбор данных:
Dataset был взят с Kaggle: https://www.kaggle.com/datasets/zerofactorialisone/rick-and-morty-discord-chatbot-dataset и в ходе разработки моделей претерпел некоторые изменения! Изначально датасет был на английском языке, но я захотел, чтобы мой бот Рик был русскоязычным, поэтому, для того чтобы сократить время на перевод я воспользовался нейронной сетью DeepL Translate, и сохранил результат в docx формат. Далее была проведена предобработка данных в ноутбуке EDA.ipynb. Для установления характера Рика, я выяснил, какими словами он пользуется чаще всего, а также обратил внимание на манеру его речи и ведение диалога! После этого сформировал итоговый датасет, который находится в папке data\data_OUT\rick.csv
![rick_wordcloud](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/b85ee4c8-4586-46bb-831d-c7e4443f0900)

# Разработка модели:
При разработке было построено 2 модели:
Первая модель была построена на принципе bag_of_words и содержится в bag_of_words_model.ipynb, а вторая модель была построена на основе cross_encoder и содержится в файле cross_encoder_model.py. Была взята за основу предобученная модель bert-base-uncased.
## Результаты обучения второй модели: 
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/1bc588b3-5ffe-4802-b03b-434c4c36a95d)
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/f60ae378-4dd6-4707-9cba-1d974ec09017)

## Выводы по результатам модели:
Данные результаты указывают на очень высокую эффективность модели в процессе обучения и валидации. Стабильная точность 100% на обучающем и валидационном наборах данных может свидетельствовать о следующем:
1.	Переобучение: модель идеально подстроилась под тренировочные данные, что может привести к снижению ее способности к обобщению на новых данных. Однако стабильно высокая точность на валидационном наборе может говорить о том, что данные хорошо представляют общую закономерность, которую модель смогла выучить.
2.	Качество данных: возможно, датасет не достаточно разнообразен или слишком мал, что позволяет модели легко достичь высокой точности. В таком случае, важно обеспечить большую вариативность и объем обучающих данных.
Я склонен придерживаться второго мнения, т.к. итоговый набор данных действительно не большой. В силу отсутствия времени больший набор собрать не удалось!
# Код инференса модели:
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/1eb787f4-bf9b-4c33-af98-a04ae31a2d82)
При тестировании модели на примере вопроса "Как тебя зовут?", модель смогла с высокой уверенностью (98%) выбрать подходящий ответ, что свидетельствует о хорошей способности модели обрабатывать естественный язык и выдавать релевантные ответы на поставленные вопросы.

# Ускорение инференса модели:
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/e10c4200-008b-430f-b48e-a442ad8faee7)
После процесса квантования модели, точность на валидационном наборе данных осталась стабильно высокой (100%). Это говорит о том, что квантование не оказало заметного негативного влияния на производительность модели, что является значительным достижением, учитывая сокращение занимаемых ресурсов и увеличение скорости инференса.
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/f100c409-f04e-4bad-98b5-8afd56e51ade)

# Запуск бота:
Данный процесс реализован через FastAPI.
Чтобы запустить бота, нужно:
1. установить ngrok.exe с сайта https://ngrok.com/ локально на ПК
2. зарегистрироваться
3. после установки в появившемся окне Your Authtoken
4. в файле settings.py WEBHOOK_HOST на ваш token
5. Далее запустить файл my_main.py

После этого можно найти в телеграмме бота rick_pickle_bot и общаться с ним!
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/054d7b14-cb91-4096-9968-468f989adbb0)

# Заключение
На основе проведенного анализа можно сделать вывод о высокой эффективности разработанной модели для задачи автоматизированного чат-бота. Однако для обеспечения более глубокого понимания ее способностей и ограничений необходимо провести дополнительные эксперименты, включая тестирование на более разнообразном и объемном наборе данных, а также оценку способности модели к обобщению на новых, невиданных примерах.


