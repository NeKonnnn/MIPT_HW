{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlGqaxJQNJZM"
   },
   "source": [
    "# Модуль 3. Мультимодальные и мультизадачные модели. Часть 1\n",
    "\n",
    "Это вторая часть домашней работы №3 \"Реализация Visual Question Answering / Document Question Answering\"\n",
    "\n",
    "## Часть 2. Использование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlE7tLFFqwdb"
   },
   "source": [
    "**Цель:** отработать навыки адаптации готовых моделей для решения прикладной задачи на русском языке, а также создание небольших демо для задач.\n",
    "\n",
    "**В каком виде прислать результат:**\n",
    "\n",
    "заполненный jupyter-notebook и видеозаписи работы с демо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4kOb23dygOQ"
   },
   "source": [
    "### [2 балла] Добавить модель переводчика\n",
    "\n",
    "У вас уже есть готовая модель, которая может по картинке отвечать на текстовые запросы к картинке. Ваша цель --- обобщить эту модель на русский язык, добавив модель переводчик, которая будет переводить запрос на русском языке в запрос на английском языке и передавать его модели. За основу вы можете взять языковую модель (например, https://huggingface.co/Helsinki-NLP/opus-mt-ru-en). Альтернативой может стать реализация функции, делающий api вызов, к приложению переводчика (например, https://libretranslate.com/).\n",
    "\n",
    "---\n",
    "\n",
    "**Ожидаемый результат**\n",
    "\n",
    "В качестве результата в этой секции вам нужно предоставить функции, которые делают перевод с русского на английского и делает инференс модели DocVQA и выводит ответ на русском языке. (В качестве примеров вопросов, можете использовать данные из датасета)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7b5TIJ1eIJT",
    "outputId": "67a55ae6-8b0b-4d5f-8ade-1b119e6ac106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "detectron2 0.6 requires pycocotools>=2.0.2, but you have pycocotools 2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
    "!pip install -q opencv-python\n",
    "!pip install -q git+https://github.com/facebookresearch/detectron2.git\n",
    "!pip install -q gradio\n",
    "!pip install -q pillow\n",
    "!pip install -q SpeechRecognition pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331,
     "referenced_widgets": [
      "b135e6812542497e8b6a07fd277a1727",
      "35f897dcf76641f0a4558a3d4c6b808d",
      "63ce7da0fd2147ccbc75f022d8a12d30",
      "434f4645e41e49fcbe1f870cd3092b8a",
      "3c02b83a20ce490a9c61a3799cf8ef03",
      "a3a10f1d727642d78b235302a47abd9b",
      "d70aa1de930b47428e1b3c4fdd9bf290",
      "673e658288c841cd92ba994900dab68a",
      "953890f9874e45fc88313069ab3c2029",
      "bc6f62ee09db47198027440183558439",
      "12f66fe9c54c4d05b4b7971e0cb3552e",
      "c672e3e22594403dad261f747f900a4a",
      "27793077320f4f4b8780ae4d9428ec01",
      "f5b3b3c32906433c9aa7132d6f52cd04",
      "62b7cb0c652545e2a9ff261d11563215",
      "24371ae9ba1c4d538688298f8db5f387",
      "869da06fe21e495e90d005ee86749a67"
     ]
    },
    "id": "ilwkoHs5f4Ih",
    "outputId": "f159b0a0-9c16-4dc4-e23b-a87982dd29f9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b135e6812542497e8b6a07fd277a1727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import MarianMTModel\n",
    "from transformers import MarianTokenizer\n",
    "from transformers import LayoutLMv2Processor\n",
    "from transformers import LayoutLMv2ForQuestionAnswering\n",
    "from transformers import LayoutLMv2Tokenizer\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import gradio as gr\n",
    "import speech_recognition as sr\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zp9AVZEjxKiT"
   },
   "outputs": [],
   "source": [
    "# Загрузка моделей и токенизаторов для обоих направлений перевода\n",
    "models = {\n",
    "    \"ru-en\": {\n",
    "        \"model_name\": \"Helsinki-NLP/opus-mt-ru-en\",\n",
    "        \"model\": None,\n",
    "        \"tokenizer\": None\n",
    "    },\n",
    "    \"en-ru\": {\n",
    "        \"model_name\": \"Helsinki-NLP/opus-mt-en-ru\",\n",
    "        \"model\": None,\n",
    "        \"tokenizer\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Функция инициализации модели и токенизатора (если они еще не были загружены)\n",
    "def load_model_and_tokenizer(direction):\n",
    "    if models[direction][\"model\"] is None or models[direction][\"tokenizer\"] is None:\n",
    "        models[direction][\"model\"] = MarianMTModel.from_pretrained(models[direction][\"model_name\"])\n",
    "        models[direction][\"tokenizer\"] = MarianTokenizer.from_pretrained(models[direction][\"model_name\"])\n",
    "\n",
    "# Функция для перевода текста\n",
    "def translate(text, direction):\n",
    "    load_model_and_tokenizer(direction)\n",
    "    tokenizer = models[direction][\"tokenizer\"]\n",
    "    model = models[direction][\"model\"]\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey45uaNTxKlW",
    "outputId": "0600b82b-1b84-4b11-a385-056ae3f8a914"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальный текст RU: Давайте поговорим об искусственном интеллекте!\n",
      "Переведенный текст RU-EN: Let's talk about artificial intelligence!\n",
      "-------------------------------------------------------------------------\n",
      "Оригинальный текст EN: Let's talk about artificial intelligence!\n",
      "Переведенный текст EN-RU: Давайте поговорим об искусственном интеллекте!\n"
     ]
    }
   ],
   "source": [
    "# Проверка функций перевода\n",
    "text_ru = \"Давайте поговорим об искусственном интеллекте!\"\n",
    "text_en = \"Let's talk about artificial intelligence!\"\n",
    "\n",
    "translated_text_ru_en = translate(text_ru, \"ru-en\")\n",
    "translated_text_en_ru = translate(text_en, \"en-ru\")\n",
    "\n",
    "print(\"Оригинальный текст RU:\", text_ru)\n",
    "print(\"Переведенный текст RU-EN:\", translated_text_ru_en)\n",
    "print('-'*73)\n",
    "print(\"Оригинальный текст EN:\", text_en)\n",
    "print(\"Переведенный текст EN-RU:\", translated_text_en_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4AxuAl3fxKrG"
   },
   "outputs": [],
   "source": [
    "# Загрузка модели, процессора и токенезатора\n",
    "model_docvqa = LayoutLMv2ForQuestionAnswering.from_pretrained(\"NeKonnn/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
    "processor_docvqa = LayoutLMv2Processor.from_pretrained(\"NeKonnn/layoutlmv2-base-uncased_finetuned_docvqa\")\n",
    "tokenizer_docvqa = LayoutLMv2Tokenizer.from_pretrained(\"NeKonnn/layoutlmv2-base-uncased_finetuned_docvqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mNfXQgIvxKuC"
   },
   "outputs": [],
   "source": [
    "def ru_inference(image_path, question_ru):\n",
    "    '''\n",
    "    Эта функция выполняет обработку изображения и вопроса на русском языке,\n",
    "    использует модель для извлечения ответа на заданный вопрос из изображения,\n",
    "    и возвращает ответ на русском языке.\n",
    "\n",
    "    Параметры:\n",
    "    image_path: путь к изображению, на котором нужно найти ответ.\n",
    "    question_ru: вопрос на русском языке, на который нужно ответить.\n",
    "\n",
    "    Возвращает:\n",
    "    answer_ru: ответ на вопрос, извлеченный из изображения, на русском языке.\n",
    "    '''\n",
    "\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    question_en = translate(question_ru, \"ru-en\")\n",
    "    inputs = processor_docvqa(image, question_en, return_tensors=\"pt\")\n",
    "    outputs = model_docvqa(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Игнорирование токенов CLS, SEP и PAD для начала и конца ответа\n",
    "    ignore_index = [tokenizer_docvqa.cls_token_id, tokenizer_docvqa.sep_token_id, tokenizer_docvqa.pad_token_id]\n",
    "    answer_start_scores[:, ignore_index] = -float(\"Inf\")\n",
    "    answer_end_scores[:, ignore_index] = -float(\"Inf\")\n",
    "\n",
    "    # Выбор начального и конечного токенов ответа\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Преобразование индексов в токены и очистка от служебных символов\n",
    "    answer_tokens = tokenizer_docvqa.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
    "    answer_tokens = [token for token in answer_tokens if token not in (tokenizer_docvqa.cls_token, tokenizer_docvqa.sep_token, tokenizer_docvqa.pad_token)]\n",
    "\n",
    "    # Сборка токенов в строку ответа\n",
    "    answer = tokenizer_docvqa.convert_tokens_to_string(answer_tokens)\n",
    "    answer_ru = translate(answer, \"en-ru\")\n",
    "\n",
    "    return answer_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOSIJvvuw7tL",
    "outputId": "833909a7-228b-466b-acbc-953e7942a221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-9m0knvur\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-9m0knvur\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 337ca3490fa7879ceeeadf6c2b73d67504ff4b4f\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (3.7.1)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.0.7)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.2.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (4.66.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.14.1)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (0.1.9)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (1.3.2)\n",
      "Requirement already satisfied: black in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6) (23.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (0.11.2)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (3.11.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->detectron2==0.6) (4.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (67.7.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->detectron2==0.6) (3.2.2)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets\n",
    "! pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!sudo apt install tesseract-ocr\n",
    "!pip install -q pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaJvYevnxKzT",
    "outputId": "87081f5d-c34a-4530-b2d8-0a19a2b0caeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: Компьютерные системы или устройства для доступа к информации. Информационные технологии оказывают значительное влияние на нашу повседневную жизнь. Информационные технологии используются всеми предприятиями вплоть до одного человека и местных операций. Глобальные компании используют их для управления данными и модернизации своих процессов. Даже продавцы смартфонов используют для сбора платежей, а уличные исполнители дают имя для сбора пожертвований. Если вы используете таблицу для каталога, который вы покупаете Рождеством, вы используете информационную технологию.\n"
     ]
    }
   ],
   "source": [
    "answer = ru_inference(\"/content/sample_data/text2.jpg\", \"Что такое информационные технологии?\")\n",
    "print(\"Ответ:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXfhzF9c0QoT"
   },
   "source": [
    "### [2 балла] Сделать демо на gradio\n",
    "\n",
    "Модель готова! Теперь было бы круто, если модель можно было захостить и оттестировать на практике. В этом задании вам нужно будет реализовать демо на gradio, которое будет принимать изображение и вопрос, а далее выдавать ответ. Пример демо, аналогично которому вам нужно реализовать модель --- https://huggingface.co/spaces/nielsr/comparing-VQA-models.\n",
    "\n",
    "\n",
    "**Подсказка:**\n",
    "\n",
    "В вкладке `Files` на демо вы можете посмотреть реализацию, там нужно заменить инференс, используемой модели, на инференс нашей модели с переводом\n",
    "\n",
    "\n",
    "**Ожидаемый результат**\n",
    "\n",
    "В качестве результата в этой секции вам нужно код для запуска демо на градио и видеозапись его работы, где реализован описанный выше функционал. Видео прикрепляйте отдельным файлом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aN1WYIX-8o5R"
   },
   "outputs": [],
   "source": [
    "def ru_inference(image, question_ru):\n",
    "    '''\n",
    "    Функция для обработки изображения и вопроса на русском языке,\n",
    "    для извлечения ответа на заданный вопрос с помощью модели документного вопросно-ответного анализа (DocVQA).\n",
    "    Ответ извлекается из текста на изображении и переводится на русский язык.\n",
    "\n",
    "    Параметры:\n",
    "    image: изображение в формате PIL.Image, на котором нужно найти ответ.\n",
    "    question_ru: вопрос на русском языке, на который нужно получить ответ из изображения.\n",
    "\n",
    "    Возвращает:\n",
    "    answer_ru: ответ на вопрос, извлечённый из текста на изображении и переведённый на русский язык.\n",
    "    '''\n",
    "\n",
    "    # Преобразование изображения в тензор PyTorch\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "\n",
    "    question_en = translate(question_ru, \"ru-en\")\n",
    "    inputs = processor_docvqa(image, question_en, return_tensors=\"pt\")\n",
    "    outputs = model_docvqa(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # Игнорирование служебных токенов при выборе начального и конечного индексов\n",
    "    ignore_index = [tokenizer_docvqa.cls_token_id, tokenizer_docvqa.sep_token_id, tokenizer_docvqa.pad_token_id]\n",
    "    answer_start_scores[:, ignore_index] = -float(\"Inf\")\n",
    "    answer_end_scores[:, ignore_index] = -float(\"Inf\")\n",
    "\n",
    "    # Выбор начального и конечного токенов для ответа\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Извлечение токенов ответа\n",
    "    answer_tokens = tokenizer_docvqa.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
    "    answer_tokens = [token for token in answer_tokens if token not in (tokenizer_docvqa.cls_token, tokenizer_docvqa.sep_token, tokenizer_docvqa.pad_token)]\n",
    "\n",
    "    # Преобразование токенов в строку ответа\n",
    "    answer = tokenizer_docvqa.convert_tokens_to_string(answer_tokens)\n",
    "    answer_ru = translate(answer, \"en-ru\")\n",
    "\n",
    "    return answer_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "kQTZRCZCXY4L",
    "outputId": "cdc0e341-253b-4503-9c7a-a4200661892c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Running on public URL: https://dfad1531182258e1c1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dfad1531182258e1c1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://dfad1531182258e1c1.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определение интерфейса Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=ru_inference,\n",
    "    inputs=[gr.Image(type=\"pil\"), gr.Textbox(lines=2, placeholder=\"Введите вопрос на русском языке\")],\n",
    "    outputs=gr.Textbox(),\n",
    ")\n",
    "\n",
    "# Запуск Gradio интерфейса\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJPiZ_u5UgpI"
   },
   "source": [
    "### [4 балла] Ответы на вопросы голосом\n",
    "\n",
    "Демо готово! Но кто хочет писать вопросы текстом?\n",
    "Здесь вам предстоить улучшить ваше демо, чтобы оно могло принимать вопросы голосом. За основу вам предлагается рассмотреть демо https://www.gradio.app/guides/real-time-speech-recognition и добавить соответствуещее окошко в ваше демо. Также вы можете добавить text-to-speech модель, чтобы оно озвучило текстовый ответ (дополнительный балл к оценке)\n",
    "\n",
    "---\n",
    "\n",
    "**Ожидаемый результат**\n",
    "\n",
    "В качестве результата в этой секции вам нужно код для запуска демо на градио и видеозапись его работы, где реализован описанный выше функционал."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-sSgkA3C3Ga",
    "outputId": "f7793f72-6ba1-44f1-959d-6a6a6ec932a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gTTS\n",
      "  Downloading gTTS-2.4.0-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2023.7.22)\n",
      "Installing collected packages: gTTS\n",
      "Successfully installed gTTS-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "qu-xHxq-CjGq"
   },
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки для TTS\n",
    "import gtts\n",
    "import tempfile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "EU-l4Wl2_ueV"
   },
   "outputs": [],
   "source": [
    "def recognize_speech(audio_path):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.AudioFile(audio_path) as source:\n",
    "        audio = r.record(source)\n",
    "    text = r.recognize_google(audio, language='ru-RU')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "F2gXGyfbClnZ"
   },
   "outputs": [],
   "source": [
    "# Функция преобразования текста в речь\n",
    "def text_to_speech(text, lang='ru'):\n",
    "    \"\"\"\n",
    "    Преобразование данного текста в аудио.\n",
    "\n",
    "    Параметры:\n",
    "    text (str): Текст для преобразования в речь.\n",
    "    lang (str): Языковой код для TTS.\n",
    "\n",
    "    Возвращает:\n",
    "    BytesIO: Буфер байтов аудиофайла.\n",
    "    \"\"\"\n",
    "    tts = gtts.gTTS(text=text, lang=lang)\n",
    "    # Создаем временный файл для сохранения аудио\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n",
    "    tts.save(temp_file.name)\n",
    "    return temp_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "myfCYFQW_uhL"
   },
   "outputs": [],
   "source": [
    "# Функция обработки изображения и вопроса\n",
    "def ru_inference(image, question_ru=None, audio=None):\n",
    "    # Преобразование изображения в тензор PyTorch\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    if audio is not None:\n",
    "        # Если предоставлен аудиофайл, используем функцию распознавания речи\n",
    "        question_ru = recognize_speech(audio)\n",
    "\n",
    "    \n",
    "    question_en = translate(question_ru, \"ru-en\")\n",
    "\n",
    "    # Препроцессинг изображения и вопроса\n",
    "    inputs = processor_docvqa(image, question_en, return_tensors=\"pt\")\n",
    "    outputs = model_docvqa(**inputs)\n",
    "\n",
    "    # Игнорирование служебных токенов и извлечение ответа\n",
    "    ignore_index = [tokenizer_docvqa.cls_token_id, tokenizer_docvqa.sep_token_id, tokenizer_docvqa.pad_token_id]\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    answer_start_scores[:, ignore_index] = -float(\"Inf\")\n",
    "    answer_end_scores[:, ignore_index] = -float(\"Inf\")\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Извлечение токенов ответа и преобразование их в строку\n",
    "    answer_tokens = tokenizer_docvqa.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
    "    answer_tokens = [token for token in answer_tokens if token not in (tokenizer_docvqa.cls_token, tokenizer_docvqa.sep_token, tokenizer_docvqa.pad_token)]\n",
    "    answer = tokenizer_docvqa.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    # Перевод ответа с английского на русский\n",
    "    answer_ru = translate(answer, \"en-ru\")\n",
    "    answer_audio_path = text_to_speech(answer_ru)\n",
    "\n",
    "    return answer_ru, answer_audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ISx8z6fI_uj_",
    "outputId": "6adcbaed-5b34-40fa-b2d3-5e780b6f91e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Running on public URL: https://4302736649b4d61985.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4302736649b4d61985.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1106, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 91, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 146, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/middleware/asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 274, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastapi/routing.py\", line 191, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 661, in queue_data\n",
      "    blocks._queue.attach_data(body)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 132, in attach_data\n",
      "    raise ValueError(\"Event not found\", event_id)\n",
      "ValueError: ('Event not found', 'dddec5aa0b704a9491548aed652e0c39')\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\", line 708, in recognize_google\n",
      "    response = urlopen(request, timeout=self.operation_timeout)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "    response = self.parent.error(\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 400: Bad Request\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 427, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1484, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1106, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 665, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"<ipython-input-63-9fa41577627e>\", line 9, in ru_inference\n",
      "    question_ru = recognize_speech(audio)\n",
      "  File \"<ipython-input-43-82054f3c6635>\", line 5, in recognize_speech\n",
      "    text = r.recognize_google(audio, language='ru-RU')\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\", line 710, in recognize_google\n",
      "    raise RequestError(\"recognition request failed: {}\".format(e.reason))\n",
      "speech_recognition.exceptions.RequestError: recognition request failed: Bad Request\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\", line 708, in recognize_google\n",
      "    response = urlopen(request, timeout=self.operation_timeout)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "    response = self.parent.error(\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 400: Bad Request\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 427, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1484, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1106, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 665, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"<ipython-input-63-9fa41577627e>\", line 9, in ru_inference\n",
      "    question_ru = recognize_speech(audio)\n",
      "  File \"<ipython-input-43-82054f3c6635>\", line 5, in recognize_speech\n",
      "    text = r.recognize_google(audio, language='ru-RU')\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\", line 710, in recognize_google\n",
      "    raise RequestError(\"recognition request failed: {}\".format(e.reason))\n",
      "speech_recognition.exceptions.RequestError: recognition request failed: Bad Request\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 472, in process_events\n",
      "    response = await self.call_prediction(awake_events, batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 436, in call_prediction\n",
      "    raise Exception(str(error) if show_error else None) from error\n",
      "Exception: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7861 <> https://4302736649b4d61985.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание интерфейса Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=ru_inference,\n",
    "    inputs=[\n",
    "        gr.Image(type=\"pil\"),\n",
    "        gr.Textbox(lines=2, placeholder=\"Введите вопрос на русском языке (или оставьте пустым, если хотите использовать голос)\"),\n",
    "        gr.Audio(sources=\"microphone\", type=\"filepath\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Ответ на русском языке\"),\n",
    "        gr.Audio(label=\"Озвученный ответ\")\n",
    "    ],\n",
    "    title=\"Интерфейс для вопросов и ответов\",\n",
    "    description=\"Загрузите изображение и введите или скажите свой вопрос.\"\n",
    ")\n",
    "\n",
    "# Запуск интерфейса Gradio\n",
    "iface.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12f66fe9c54c4d05b4b7971e0cb3552e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24371ae9ba1c4d538688298f8db5f387": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27793077320f4f4b8780ae4d9428ec01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35f897dcf76641f0a4558a3d4c6b808d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_673e658288c841cd92ba994900dab68a",
      "placeholder": "​",
      "style": "IPY_MODEL_953890f9874e45fc88313069ab3c2029",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "3c02b83a20ce490a9c61a3799cf8ef03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_f5b3b3c32906433c9aa7132d6f52cd04",
      "style": "IPY_MODEL_62b7cb0c652545e2a9ff261d11563215",
      "tooltip": ""
     }
    },
    "434f4645e41e49fcbe1f870cd3092b8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_c672e3e22594403dad261f747f900a4a",
      "style": "IPY_MODEL_27793077320f4f4b8780ae4d9428ec01",
      "value": true
     }
    },
    "62b7cb0c652545e2a9ff261d11563215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "63ce7da0fd2147ccbc75f022d8a12d30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_bc6f62ee09db47198027440183558439",
      "placeholder": "​",
      "style": "IPY_MODEL_12f66fe9c54c4d05b4b7971e0cb3552e",
      "value": ""
     }
    },
    "673e658288c841cd92ba994900dab68a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869da06fe21e495e90d005ee86749a67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "953890f9874e45fc88313069ab3c2029": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3a10f1d727642d78b235302a47abd9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24371ae9ba1c4d538688298f8db5f387",
      "placeholder": "​",
      "style": "IPY_MODEL_869da06fe21e495e90d005ee86749a67",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "b135e6812542497e8b6a07fd277a1727": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35f897dcf76641f0a4558a3d4c6b808d",
       "IPY_MODEL_63ce7da0fd2147ccbc75f022d8a12d30",
       "IPY_MODEL_434f4645e41e49fcbe1f870cd3092b8a",
       "IPY_MODEL_3c02b83a20ce490a9c61a3799cf8ef03",
       "IPY_MODEL_a3a10f1d727642d78b235302a47abd9b"
      ],
      "layout": "IPY_MODEL_d70aa1de930b47428e1b3c4fdd9bf290"
     }
    },
    "bc6f62ee09db47198027440183558439": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c672e3e22594403dad261f747f900a4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d70aa1de930b47428e1b3c4fdd9bf290": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "f5b3b3c32906433c9aa7132d6f52cd04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
