{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2_2YjnOFbds"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.28.0 datasets evaluate ftfy > null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWMaTvxMFY_j"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import ftfy\n",
    "import evaluate\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oz2sFPFD9BZz"
   },
   "outputs": [],
   "source": [
    "DIR_PATH = '/content/'\n",
    "MODEL_SAVE_PATH = DIR_PATH + 'sentiment_model'\n",
    "TRAIN_DATA_PATH = DIR_PATH + 'train.csv'\n",
    "BATCH_SIZE = 334\n",
    "MAX_LENGTH = 48\n",
    "MODEL_NAME = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sGy9CJC6_64"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/train.csv\")[['Text', 'Sentiment']]\n",
    "df = df.dropna()\n",
    "df.columns = ['text', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOLrooM17jOU"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI-BQnsY750Z",
    "outputId": "594e5614-013b-45dc-bbf2-aa0665d1735c"
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWMYjZT64Sls"
   },
   "outputs": [],
   "source": [
    "valid_split = int(0.9*df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NX07lQQiGNBJ",
    "outputId": "1761410e-e1b3-4f04-d994-1b13ce976384"
   },
   "outputs": [],
   "source": [
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df[:valid_split]),\n",
    "    \"valid\": Dataset.from_pandas(df[valid_split:])\n",
    "    })\n",
    "\n",
    "unique_sorted_labels = sorted(list(set(ds['train']['label'])))\n",
    "id2label = dict([(i,tag) for (i, tag) in enumerate(unique_sorted_labels)])\n",
    "label2id = dict([(tag,i) for (i, tag) in id2label.items()])\n",
    "print(f\"{len(id2label)} label\")\n",
    "print(id2label)\n",
    "\n",
    "\n",
    "def preprocess_text(text, translate=True):\n",
    "    lower_fixed_text = ftfy.fix_text(text.lower().strip())\n",
    "    return lower_fixed_text\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    \"\"\"\n",
    "    fix encodings, lowering, striping whitespaces\n",
    "    \"\"\"\n",
    "    batch['text'] = [preprocess_text(text.lower()) for text in batch['text']]\n",
    "    batch[\"label\"] = [label2id[label] for label in batch[\"label\"]]\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYoRI-2ZGmYK"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK0yPc6qI0Ce"
   },
   "source": [
    "## Preprocess and vectorize texts\n",
    "\n",
    "from :\n",
    "\n",
    "We use the `ftfy` library to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the `spaCy` tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSN5ncnKGct2"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, translate=True):\n",
    "    lower_fixed_text = ftfy.fix_text(text.lower().strip())\n",
    "    return lower_fixed_text\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    \"\"\"\n",
    "    fix encodings, lowering, striping whitespaces\n",
    "    \"\"\"\n",
    "    batch['text'] = [preprocess_text(text.lower()) for text in batch['text']]\n",
    "    tokenizer_batch = tokenizer(batch[\"text\"], padding=True, truncation=True,\n",
    "                                max_length=MAX_LENGTH)\n",
    "    tokenizer_batch[\"label\"] = [label2id[label] for label in batch[\"label\"]]\n",
    "    return tokenizer_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWeD_11KGPbm",
    "outputId": "bf4b91de-e137-460d-d85f-9fc657ef8fe4"
   },
   "outputs": [],
   "source": [
    "tokenized_ds = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8CG2nmkGDq6",
    "outputId": "f64d7ee1-e763-4d55-a30a-0b0459a05c21"
   },
   "outputs": [],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6wcg4fuIxen"
   },
   "source": [
    "collate data with panding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3Hg_zVDIxG7"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJt7DMdLI_eg"
   },
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRGsdaDqJAl3"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0aBojqBiX9a",
    "outputId": "2105d58d-4bb1-4ee6-c2ab-dc1ad6a86613"
   },
   "outputs": [],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVQKXvrCJaQh"
   },
   "source": [
    "define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0R0_SEJJa_n",
    "outputId": "2245670c-8d89-4037-8685-23e6e755a13d"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(id2label),\n",
    "    id2label=id2label, label2id=label2id, ignore_mismatched_sizes = True\n",
    ")\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/due_model_14/checkpoint-7125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6-Eew4A4MCL"
   },
   "source": [
    "define a trainer for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWPfrZqLJlFh"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_PATH,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    #load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzAEX80_4RCS"
   },
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnbONfB2KFMy",
    "outputId": "628aac11-c194-45ec-cc8e-7665867dfc02"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "time_ellapsed = end_time - start_time\n",
    "print(time_ellapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tdfkKPCaF0U"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"sentiment_model\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euR1wB9zIktg"
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\",\n",
    "                model=\"/kaggle/output/sentiment_model\",\n",
    "                tokenizer='/kaggle/output/tokenizer')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/unit-3-nlp-txt-classification/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "preds = []\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    preds.append((row['id'], pipe(row['Text'])[0]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_dict = {'id': [pred[0] for pred in preds],\n",
    "              'Sentiment': [pred[1] for pred in preds]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(submit_dict).to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
