{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f213e0a7",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e65c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00882b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nekon\\AppData\\Local\\Temp\\ipykernel_3340\\3536822836.py:22: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import Hyperband\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from kerastuner.tuners import Hyperband\n",
    "from kerastuner import HyperParameters\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8eb281",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ae906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "submission_data = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87799ce",
   "metadata": {},
   "source": [
    "# Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee624e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                               Text  \\\n",
       "0          0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   \n",
       "1          1  advice Talk to your neighbours family to excha...   \n",
       "2          2  Coronavirus Australia: Woolworths to give elde...   \n",
       "3          3  My food stock is not the only one which is emp...   \n",
       "4          4  Me, ready to go at supermarket during the #COV...   \n",
       "\n",
       "            Sentiment  \n",
       "0             Neutral  \n",
       "1            Positive  \n",
       "2            Positive  \n",
       "3            Positive  \n",
       "4  Extremely Negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296bb6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  787bc85b-20d4-46d8-84a0-562a2527f684   \n",
       "1  17e934cd-ba94-4d4f-9ac0-ead202abe241   \n",
       "2  5914534b-2b0f-4de8-bb8a-e25587697e0d   \n",
       "3  cdf06cfe-29ae-48ee-ac6d-be448103ba45   \n",
       "4  aff63979-0256-4fb9-a2d9-86a3d3ca5470   \n",
       "\n",
       "                                                Text  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...  \n",
       "2  Find out how you can protect yourself and love...  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01184c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>787bc85b-20d4-46d8-84a0-562a2527f684</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17e934cd-ba94-4d4f-9ac0-ead202abe241</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5914534b-2b0f-4de8-bb8a-e25587697e0d</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdf06cfe-29ae-48ee-ac6d-be448103ba45</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aff63979-0256-4fb9-a2d9-86a3d3ca5470</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id Sentiment\n",
       "0  787bc85b-20d4-46d8-84a0-562a2527f684   Neutral\n",
       "1  17e934cd-ba94-4d4f-9ac0-ead202abe241   Neutral\n",
       "2  5914534b-2b0f-4de8-bb8a-e25587697e0d   Neutral\n",
       "3  cdf06cfe-29ae-48ee-ac6d-be448103ba45   Neutral\n",
       "4  aff63979-0256-4fb9-a2d9-86a3d3ca5470   Neutral"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3c0a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7711\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка распределения столбца 'Sentiment' в тренировочном датасете\n",
    "sentiment_distribution = train_data['Sentiment'].value_counts()\n",
    "sentiment_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3d599c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверим наличие пропущенных значений в колонке \"Text\" обучающего и тестового набора данных\n",
    "missing_train = train_data[\"Text\"].isnull().sum()\n",
    "missing_test = test_data[\"Text\"].isnull().sum()\n",
    "\n",
    "missing_train, missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d0fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление строк с пропущенными значениями\n",
    "train_data = train_data.dropna(subset=[\"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807fb7d6",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023fe7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Processed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>menyrbie phil_gahan chrisitv and and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>advice talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>coronavirus australia woolworths to give elder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>my food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>me ready to go at supermarket during the covid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   \n",
       "1  advice Talk to your neighbours family to excha...   \n",
       "2  Coronavirus Australia: Woolworths to give elde...   \n",
       "3  My food stock is not the only one which is emp...   \n",
       "4  Me, ready to go at supermarket during the #COV...   \n",
       "\n",
       "                                      Processed_Text  \n",
       "0              menyrbie phil_gahan chrisitv and and   \n",
       "1  advice talk to your neighbours family to excha...  \n",
       "2  coronavirus australia woolworths to give elder...  \n",
       "3  my food stock is not the only one which is emp...  \n",
       "4  me ready to go at supermarket during the covid...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Предобработка текста: преобразование в нижний регистр, удаление URL, знаков препинания и т.д.\n",
    "    \"\"\"\n",
    "    # Преобразование в нижний регистр\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление URL\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Удаление знаков препинания и чисел\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) \n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Предобработка текстовых данных\n",
    "train_data[\"Processed_Text\"] = train_data[\"Text\"].apply(preprocess_text)\n",
    "test_data[\"Processed_Text\"] = test_data[\"Text\"].apply(preprocess_text)\n",
    "\n",
    "train_data[[\"Text\", \"Processed_Text\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de74b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41158, 62), (3798, 62))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация токенизатора\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data[\"Processed_Text\"])\n",
    "\n",
    "# Преобразование текста в последовательности чисел\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data[\"Processed_Text\"])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data[\"Processed_Text\"])\n",
    "\n",
    "# Определение максимальной длины последовательности\n",
    "max_length = max([len(seq) for seq in train_sequences])\n",
    "\n",
    "# Дополнение последовательностей до одной и той же длины\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "train_padded.shape, test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67cf2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Extremely Negative', 'Extremely Positive', 'Negative', 'Neutral',\n",
       "       'Positive', nan], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Инициализация и обучение LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "train_labels = encoder.fit_transform(train_data[\"Sentiment\"])\n",
    "\n",
    "# Проверка уникальных классов\n",
    "unique_classes = encoder.classes_\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e382d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Загрузка предварительно обученных векторов слов \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload_word2vec_format(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39mfvocab, binary\u001b[38;5;241m=\u001b[39mbinary, encoding\u001b[38;5;241m=\u001b[39mencoding, unicode_errors\u001b[38;5;241m=\u001b[39municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit, datatype\u001b[38;5;241m=\u001b[39mdatatype, no_header\u001b[38;5;241m=\u001b[39mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2045\u001b[0m             counts[word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(count)\n\u001b[0;32m   2047\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading projection weights from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fname)\n\u001b[1;32m-> 2048\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_header:\n\u001b[0;32m   2050\u001b[0m         \u001b[38;5;66;03m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[38;5;241m=\u001b[39m _shortcut_open(\n\u001b[0;32m    189\u001b[0m     uri,\n\u001b[0;32m    190\u001b[0m     mode,\n\u001b[0;32m    191\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    192\u001b[0m     buffering\u001b[38;5;241m=\u001b[39mbuffering,\n\u001b[0;32m    193\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    194\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    195\u001b[0m     newline\u001b[38;5;241m=\u001b[39mnewline,\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "# Загрузка предварительно обученных векторов слов \n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcfb32e",
   "metadata": {},
   "source": [
    "# Построение RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d9a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание матрицы весов для слоя Embedding\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 300))  # используем размерность 300 для word2vec \n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_vector = word2vec_model[word]\n",
    "        embedding_matrix[i] = embedding_vector  \n",
    "\n",
    "# Определение размера словаря и размера вложения\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Создание модели\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix],\n",
    "                              input_length=max_length, trainable=False),  \n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(train_data['Sentiment'].unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Обучение модели\n",
    "history = model.fit(train_padded, train_labels, epochs=5, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27051231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Визуализация истории обучения модели.\n",
    "    \n",
    "    Параметры:\n",
    "    - history: объект истории, возвращенный функцией model.fit().\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # График потерь\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Evolution')\n",
    "\n",
    "    # График точности\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy Evolution')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b8e29",
   "metadata": {},
   "source": [
    "Модель успешно обучается, что видно по уменьшению потерь и увеличению точности на тренировочных данных.\n",
    "\n",
    "При этом, начиная с пятой эпохи,судя по графикам, модель переобучается, т.к. валидационные потери начинают увеличиваться, а точность немного уменьшается.\n",
    "\n",
    "Рассмотрим добавление регуляризации и используем метод dropout.\n",
    "\n",
    "Также рассмотрим возможность обучения модели на большем количестве эпох с ранним остановом на основе валидационных потерь, чтобы предотвратить дальнейшее переобучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение размера словаря и размера вложения\n",
    "vocab_size = len(tokenizer.word_index) + 189\n",
    "\n",
    "# Создание модели\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix],\n",
    "                              input_length=max_length, trainable=False),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout слой\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout слой\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Dropout слой\n",
    "    tf.keras.layers.Dense(len(unique_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callback для раннего останова\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Обучение модели с добавлением callback для раннего останова\n",
    "history = model.fit(train_padded, train_labels, epochs=10, validation_split=0.2, batch_size=32, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe2654",
   "metadata": {},
   "source": [
    "Данная модель также успешно обучается, что видно по уменьшению потерь и увеличению точности на тренировочных данных.\n",
    "\n",
    "Механизм раннего останова, по всей видимости, успешно прекратил обучение, когда валидационные потери начали расти (после 6-й эпохи), предотвращая дальнейшее переобучение.\n",
    "\n",
    "Добавление Dropout и раннего останова помогли улучшить производительность модели на валидационных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc08d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение размера словаря и размера вложения\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Создание модели\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix],\n",
    "                              input_length=max_length, trainable=False),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),  # Bidirectional LSTM\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),  # Добавляем еще один слой LSTM\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(unique_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callback для раннего останова\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Обучение модели с добавлением callback для раннего останова\n",
    "history = model.fit(train_padded, train_labels, epochs=10, validation_split=0.2, batch_size=32, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на тестовых данных\n",
    "test_predictions = model.predict(test_padded)\n",
    "test_predictions = test_predictions.argmax(axis=1)\n",
    "\n",
    "# Обратное преобразование предсказанных меток в текстовые значения\n",
    "predicted_labels = encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Создание DataFrame для сохранения предсказаний\n",
    "submission_data = pd.DataFrame({\n",
    "    'id': test_data['id'],  # использование столбца 'id' из тестовых данных\n",
    "    'Sentiment': predicted_labels\n",
    "})\n",
    "\n",
    "# Сохранение предсказаний в CSV файл\n",
    "submission_data.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c6c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591fbcb",
   "metadata": {},
   "source": [
    "### Архитектура RNN, близкая к state of the art для данной задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение размера словаря и размера вложения\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "# Создание модели\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 300, weights=[embedding_matrix],\n",
    "                              input_length=max_length, trainable=False),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),  # Bidirectional LSTM\n",
    "    Dropout(0.5),  # Dropout для регуляризации\n",
    "    Bidirectional(LSTM(32, return_sequences=True)),  # Дополнительный Bidirectional LSTM слой\n",
    "    Dropout(0.5),  # Dropout для регуляризации\n",
    "    Bidirectional(LSTM(32)),  # Дополнительный Bidirectional LSTM слой\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout для регуляризации\n",
    "    Dense(len(unique_classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callback для раннего останова\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Обучение модели с добавлением callback для раннего останова\n",
    "history = model.fit(train_padded, train_labels, epochs=10, validation_split=0.2, batch_size=32, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6c34e",
   "metadata": {},
   "source": [
    "### Подбор гипперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f445c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp: HyperParameters):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Зафиксируем размер embedding слоя, используя веса из Word2Vec\n",
    "    model.add(tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=300,  # фиксированный размер вектора из Word2Vec\n",
    "        weights=[embedding_matrix],  # используем предварительно обученные веса\n",
    "        input_length=max_length,\n",
    "        trainable=False  # не обучаем этот слой во время тренировки\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=128, step=16), \n",
    "        return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=128, step=16))))\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=32, max_value=128, step=16), \n",
    "        activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(len(unique_classes), activation='softmax'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')), \n",
    "                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,\n",
    "    directory='hyperband',\n",
    "    project_name='text_classification'\n",
    ")\n",
    "\n",
    "tuner.search(train_padded, train_labels, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение информации о лучшей модели\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Вывод лучших гиперпараметров\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('dense_units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Получение и компиляция лучшей модели\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Обучение лучшей модели\n",
    "best_model.fit(train_padded, train_labels, epochs=best_hps.get('epochs'), validation_split=0.2, batch_size=best_hps.get('batch_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeaa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание на тестовых данных\n",
    "test_predictions_3 = best_model.predict(test_padded)\n",
    "test_predictions_3 = test_predictions_3.argmax(axis=1)\n",
    "\n",
    "# Обратное преобразование предсказанных меток в текстовые значения\n",
    "predicted_labels_3 = encoder.inverse_transform(test_predictions_3)\n",
    "\n",
    "# Создание DataFrame для сохранения предсказаний\n",
    "submission_data_3 = pd.DataFrame({\n",
    "    'id': test_data['id'],  # использование столбца 'id' из тестовых данных\n",
    "    'Sentiment': predicted_labels_3\n",
    "})\n",
    "\n",
    "# Сохранение предсказаний в CSV файл\n",
    "submission_data_3.to_csv('predictions_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
