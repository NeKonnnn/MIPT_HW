{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0ZpnQDkKLTL"
   },
   "source": [
    "\n",
    "В данном домашнем задании вам необходимо обучить языковую модель (не более 1B параметров) решать примеры на сложение как можно более длинных чисел.\n",
    "\n",
    "**Ожидаемый результат**\n",
    "\n",
    "Необходимо предоставить код, а также технический отчет, содержащий описание метода и используемых данных, оценку качества (точность сложения).\n",
    "\n",
    "**Пояснения**\n",
    "\n",
    "1. Можно использовать любые предобученные модели, можно их файнтюнить, обучать с нуля, адаптировать любым другим способом или брать как есть. Главное, чтобы все использованные вами идеи, код или веса моделей были описаны в приложенном отчете со ссылкой на источник.\n",
    "\n",
    "2. Мы ожидаем, что ваш код принимает на вход два числа (в виде строк их десятичной записи) и выдает ответ в любом человекочитаемом виде. Однако, если ваша модель работает с входом в виде предложения на естественном языке, это тоже нормально, главное, чтобы это было описано в отчете.\n",
    "\n",
    "3. Можно оценить качество работы алгоритма, посчитав accuracy на случайных множествах чисел разной длины. Если вам кажется более подходящей другая метрика, мы примем ваше решение. Опишите вашу метрику и аргументируйте выбор в отчете.\n",
    "\n",
    "**Подсказка**\n",
    "\n",
    "В качестве ориентира можете использовать следующий репозиторий (https://github.com/liutiedong/goat). В нём реализовано сборка датасета для обучения и само обучение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs6tGTK-NOnv"
   },
   "source": [
    "### Разбаловка\n",
    "\n",
    "- [2 балла] Сбор датасета для обучения  \n",
    "- [3 балла] Реализация скрипта модели\n",
    "- [3 балла] Создание обученной модели с качеством (100% на числах длины < 10)\n",
    "- [2 баллов] Валидация модели\n",
    "\n",
    "Для валидации можно использовать и готовую языковую модель без получения баллов за 3 пункт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание вашего решения\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install peft\n",
    "!pip install accelerate\n",
    "!pip install sentencepiece\n",
    "!pip install gradio\n",
    "!pip install appdirs\n",
    "!pip install loralib\n",
    "!pip install black\n",
    "!pip install black[jupyter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import random\n",
    "from typing import Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import TrainerCallback\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers import LlamaTokenizer\n",
    "\n",
    "from peft import LoraConfig\n",
    "from peft import LoraConfig get_peft_model\n",
    "from peft import LoraConfig get_peft_model_state_dict\n",
    "from peft import LoraConfig prepare_model_for_int8_training\n",
    "from peft import LoraConfig set_peft_model_state_dict\n",
    "from peft import LoraConfig PeftModel\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение устройства для обучения (GPU или CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "path = 'drive/MyDrive/HW_5/'\n",
    "my_model='nickypro/tinyllama-15M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем свой датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_dataset(N, output_file):\n",
    "    # Создание пар чисел различной длины\n",
    "    number_pairs = \\\n",
    "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) for i in range(1, 14) for j in range(i, 14) for _ in range(N)] +\\\n",
    "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) for i in range(3, 14) for j in range(i, 14) for _ in range(N)] +\\\n",
    "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) for i in range(6, 14) for j in range(i, 14) for _ in range(N)] +\\\n",
    "    [(random.randint(10**(i-1), 10**i), random.randint(10**(j-1), 10**j)) for i in range(9, 14) for j in range(i, 14) for _ in range(N)]\n",
    "\n",
    "    # Перемешивание пар чисел\n",
    "    random.shuffle(number_pairs)\n",
    "\n",
    "    print(\"Количество примеров на сложение:\", len(number_pairs))\n",
    "\n",
    "    addition_examples = []\n",
    "\n",
    "    for first_num, second_num in number_pairs:\n",
    "        # Случайный выбор порядка чисел\n",
    "        if random.random() < 0.5:\n",
    "            first_num, second_num = second_num, first_num\n",
    "\n",
    "        sum_result = first_num + second_num\n",
    "\n",
    "        question = f\"{first_num} + {second_num}\"\n",
    "        output = f\"{first_num} + {second_num} = {sum_result}\"\n",
    "\n",
    "        # Проверка корректности вычисления\n",
    "        assert(output.split()[-1] == str(sum_result))\n",
    "        addition_examples.append({\"input\": question, \"output\": output, \"answer\": str(sum_result),\n",
    "                                  \"instruction\": question})\n",
    "\n",
    "    # Сохранение данных в файл\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(addition_examples, f, indent=4)\n",
    "        \n",
    "generate_dataset(1000, f\"{path}dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " class TextGeneratorHelper(object):\n",
    "    # Функция для создания промпта, используемого моделью генерации текста\n",
    "    def create_text_prompt(\n",
    "        self,\n",
    "        task_description: str,\n",
    "        optional_label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        :param task_description: Описание задачи, которое будет представлено в промпте.\n",
    "        :param optional_label: Дополнительная метка, которая может быть добавлена к промпту.\n",
    "        :return: Сформированный промпт.\n",
    "        \"\"\"\n",
    "        # Формирование основы промпта\n",
    "        prompt = f\"{task_description}\\nAnswer: \"\n",
    "\n",
    "        # Добавление дополнительной метки к промпту, если он указан\n",
    "        if optional_label:\n",
    "            prompt = f\"{prompt}{optional_label}\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    # Функция для извлечения ответа из сгенерированного текста модели\n",
    "    def extract_model_response(self, model_output: str) -> str:\n",
    "        \"\"\"\n",
    "        :param model_output: Текстовый вывод модели.\n",
    "        :return: Обработанный ответ, извлеченный из вывода модели.\n",
    "        \"\"\"\n",
    "        # Замена общепринятых символов на математические символы деления и умножения\n",
    "        return model_output.split(\"Answer:\")[1].strip().replace(\"/\", \"\\u00F7\").replace(\"*\", \"\\u00D7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSaverCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Класс обратного вызова для сохранения модели в процессе обучения.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, model, output_dir):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Функция вызывается при сохранении модели. Генерирует пример запроса,\n",
    "        получает ответ от модели и выводит его.\n",
    "        \"\"\"\n",
    "        text_helper = TextGeneratorHelper()\n",
    "        prompt = text_helper.create_text_prompt(\"2851673 + 7678457\")\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "        generation_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                pad_token_id=0,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                max_new_tokens=13\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        print(self.tokenizer.decode(s, skip_special_tokens=True).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    # Параметры модели и данных\n",
    "    my_model: str = \"\",  # Укажите модель, например, --my_model='decapoda-research/llama-7b-hf'\n",
    "    data_path: str = \"dataset.json\",\n",
    "    output_dir: str = \"weights\",\n",
    "\n",
    "    # Гиперпараметры обучения\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 16,\n",
    "    num_epochs: int = 10,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 512,\n",
    "    val_set_size: int = 1000,\n",
    "\n",
    "    # Параметры LoRA\n",
    "    lora_r: int = 64,\n",
    "    lora_alpha: int = 64,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ],\n",
    "\n",
    "    # Параметры LLM\n",
    "    train_on_inputs: bool = False,  # Если False, то входы не учитываются в потерях\n",
    "    group_by_length: bool = False,  # Быстрее, но может привести к странной кривой потерь\n",
    "\n",
    "    resume_from_checkpoint: str = None,  # Чекпоинт для возобновления обучения\n",
    "):\n",
    "    # Проверка наличия указанной модели\n",
    "    assert my_model, \"Укажите --my_model, например, --my_model='decapoda-research/llama-7b-hf'\"\n",
    "\n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "\n",
    "    text_helper = TextGeneratorHelper()\n",
    "\n",
    "    # Настройка распределенного обучения, если необходимо\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps //= world_size\n",
    "\n",
    "    # Инициализация модели и токенизатора\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        my_model,\n",
    "        # load_in_8bit=True,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=device_map,\n",
    "        offload_folder=\"offload\"\n",
    "    )\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "    tokenizer.pad_token_id = 0\n",
    "    tokenizer.padding_side = \"left\"  # Позволяет пакетную обработку\n",
    "\n",
    "    # Определение функций для токенизации и генерации подсказок\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        \"\"\"\n",
    "        Токенизация текстового запроса.\n",
    "        \"\"\"\n",
    "        result = tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=cutoff_len,\n",
    "            padding=False,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "        ):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_and_tokenize_prompt(data_point):\n",
    "        \"\"\"\n",
    "        Генерация и токенизация запроса на основе данных.\n",
    "        \"\"\"\n",
    "        full_prompt = text_helper.create_text_prompt(\n",
    "            data_point[\"instruction\"],\n",
    "            data_point[\"answer\"],\n",
    "        )\n",
    "        tokenized_full_prompt = tokenize(full_prompt)\n",
    "        if not train_on_inputs:\n",
    "            user_prompt = text_helper.create_text_prompt(\n",
    "                data_point[\"instruction\"]\n",
    "            )\n",
    "            tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
    "            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
    "\n",
    "            tokenized_full_prompt[\"labels\"] = [\n",
    "                -100\n",
    "            ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
    "                user_prompt_len:\n",
    "            ]\n",
    "        return tokenized_full_prompt\n",
    "\n",
    "    # Подготовка модели для обучения с использованием INT8\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    # Конфигурация LoRA\n",
    "    config = LoraConfig(\n",
    "        r=lora_r, lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout, bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "\n",
    "    # Загрузка данных для обучения\n",
    "    if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "        data = load_dataset(\"json\", data_files=data_path)\n",
    "    else:\n",
    "        data = load_dataset(data_path)\n",
    "\n",
    "    # Возобновление обучения с чекпоинта, если указано\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint_name = os.path.join(\n",
    "            resume_from_checkpoint, \"pytorch_model.bin\"\n",
    "        )\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            checkpoint_name = os.path.join(\n",
    "                resume_from_checkpoint, \"adapter_model.bin\"\n",
    "            )\n",
    "            resume_from_checkpoint = False\n",
    "\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Возобновление обучения с {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Чекпоинт {checkpoint_name} не найден\")\n",
    "\n",
    "    model.print_trainable_parameters()  # Вывод информации о тренируемых параметрах\n",
    "\n",
    "    # Разделение данных на обучающую и валидационную выборки\n",
    "    if val_set_size > 0:\n",
    "        train_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\n",
    "        train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    else:\n",
    "        train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "        val_data = None\n",
    "\n",
    "    # Настройка и запуск обучения\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=False,\n",
    "            logging_steps=200,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=10,\n",
    "            load_best_model_at_end=False if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    "        callbacks=[ModelSaverCallback(tokenizer, model, output_dir)]\n",
    "    )\n",
    "\n",
    "    # Компиляция модели для оптимизации производительности\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    # Запуск процесса обучения\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    # Сохранение обученной модели\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    print(\"\\nЕсли выше есть предупреждение о недостающих ключах, проигнорируйте его.\")\n",
    "    \n",
    "train(my_model=my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка по метрикам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки модели буду использовать метрики MAPE и accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dataset(30, output_file=f\"{path}/validation_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предварительно обученной модели Llama для каузального языкового моделирования\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Загрузка модели Peft с конкретным чекпоинтом\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    f'{path}weights/checkpoint-16000',\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Перевод модели в режим оценки (эвалюации)\n",
    "model.eval()\n",
    "\n",
    "# Компиляция модели для оптимизации производительности\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Загрузка токенизатора Llama\n",
    "tokenizer = LlamaTokenizer.from_pretrained('hf-internal-testing/llama-tokenizer')\n",
    "\n",
    "# Установка ID токена для паддинга и настройка токенизатора\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def generate_answers(instructions, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Генерирует ответы на предоставленные инструкции с помощью модели.\n",
    "\n",
    "    :param instructions: Список инструкций для генерации ответов.\n",
    "    :param model: Модель для генерации ответов.\n",
    "    :param tokenizer: Токенизатор для обработки текста.\n",
    "    :return: Список сырых ответов.\n",
    "    \"\"\"\n",
    "    text_helper = TextGeneratorHelper()\n",
    "    raw_answers = []\n",
    "\n",
    "    # Генерация ответов для каждой инструкции\n",
    "    for instruction in tqdm(instructions):\n",
    "        prompt = text_helper.create_text_prompt(instruction)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            pad_token_id=0,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=16\n",
    "        )\n",
    "        sequence = generation_output.sequences[0]\n",
    "        raw_answers.append(tokenizer.decode(sequence, skip_special_tokens=True).strip())\n",
    "\n",
    "    return raw_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_answers, answers):\n",
    "    num = 0\n",
    "    for i in range(len(answers)):\n",
    "        if true_answers[i] == answers[i]:\n",
    "            num += 1\n",
    "    return num / len(answers)\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy': accuracy,\n",
    "    'MAPE': mean_absolute_percentage_error\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}validation_dataset.json\", 'r') as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "\n",
    "instructions = [el['instruction'] for el in validation_data]\n",
    "right_answers = [int(el['answer']) for el in validation_data]\n",
    "\n",
    "raw_answers = generate_answers(instructions, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answers([\"2851673 + 7678457\"], model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number(raw_answer):\n",
    "    \"\"\"\n",
    "    Извлекает числовое значение из ответа модели.\n",
    "\n",
    "    :param raw_answer: текстовый ответ модели.\n",
    "    :return: Целочисленное значение, извлеченное из ответа. В случае ошибки возвращает 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(\"\".join(list(filter(lambda x: x.isdigit(), raw_answer.split('\\n')[1].split(':')[1]))))\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "answers = list(map(extract_number, raw_answers))\n",
    "answers[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_by_length(instructions, right_answers, answers):\n",
    "    \"\"\"\n",
    "    Вычисляет метрики по длине для заданных инструкций и ответов.\n",
    "\n",
    "    :param instructions: Список инструкций.\n",
    "    :param right_answers: Список правильных ответов.\n",
    "    :param answers: Список ответов, полученных от модели.\n",
    "    :return: Словарь с метриками для каждой комбинации длин чисел в инструкциях.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in range(len(instructions)):\n",
    "        # Вычисление длины каждого числа в инструкции\n",
    "        lengths = tuple(map(lambda x: len(x.strip()), instructions[i].split('+')))\n",
    "\n",
    "        # Инициализация словаря для новой комбинации длин\n",
    "        if lengths not in result:\n",
    "            result[lengths] = [[], []]\n",
    "\n",
    "        # Добавление правильных и полученных ответов в соответствующие списки\n",
    "        result[lengths][0].append(right_answers[i])\n",
    "        result[lengths][1].append(answers[i])\n",
    "\n",
    "    # Вычисление метрик для каждой комбинации длин\n",
    "    result_metrics = {\n",
    "        lengths: {\n",
    "            metric: metrics[metric](result[lengths][0], result[lengths][1])\n",
    "            for metric in metrics\n",
    "        }\n",
    "        for lengths in result\n",
    "    }\n",
    "\n",
    "    return result_metrics\n",
    "\n",
    "metrics_dict = compute_metrics_by_length(instructions, right_answers, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(metrics_dict, metrics):\n",
    "    \"\"\"\n",
    "    Визуализирует несколько метрик с использованием тепловых карт.\n",
    "\n",
    "    :param metrics_dict: Словарь с метриками.\n",
    "    :param metrics: Список названий метрик для визуализации.\n",
    "    \"\"\"\n",
    "    # Создание фигуры с несколькими подграфиками\n",
    "    num_metrics = len(metrics)\n",
    "    fig, axes = plt.subplots(1, num_metrics, figsize=(num_metrics * 6, 5))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        result = visualize_metric(metrics_dict, metric)\n",
    "        sns.heatmap(result['metric'], ax=axes[i], annot=True, cmap='viridis')\n",
    "        axes[i].set_title(metric)\n",
    "        axes[i].set_xlabel('Length of Second Number (l2)')\n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel('Length of First Number (l1)')\n",
    "        else:\n",
    "            axes[i].set_ylabel('')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_metrics(metrics_dict, ['MAPE', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LC7YuaqM6n-"
   },
   "source": [
    "### *[1 балл] Дополнительное задание\n",
    "\n",
    "Реализовать хостинг вашей модели на gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(instruction):\n",
    "    \"\"\"\n",
    "    Выполняет оценку (вычисление) для заданной арифметической инструкции.\n",
    "\n",
    "    :param instruction: Текстовая строка с арифметической задачей.\n",
    "    :return: Результат выполнения арифметической задачи.\n",
    "    \"\"\"\n",
    "    # Генерация ответов для данной инструкции\n",
    "    return generate_answers([instruction], model, tokenizer)[0]\n",
    "\n",
    "# Создание веб-интерфейса с использованием Gradio\n",
    "gr.Interface(\n",
    "    fn=evaluate,  # Функция для обработки ввода\n",
    "    inputs=[  # Компоненты для ввода данных пользователем\n",
    "        gr.components.Textbox(\n",
    "            lines=1,\n",
    "            label=\"Арифметика\",\n",
    "            placeholder=\"63303235 + 20239503\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[  # Компоненты для вывода результатов\n",
    "        gr.Textbox(\n",
    "            lines=5,\n",
    "            label=\"Результат\",\n",
    "        )\n",
    "    ],\n",
    "    title=\"Арифметическая LLaMA\",  # Заголовок интерфейса\n",
    "    description=\"Эта модель - 15M llama, дообученная на задачах a+b\",  # Описание интерфейса\n",
    ").queue().launch(server_name=\"0.0.0.0\")  # Запуск веб-интерфейса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Lpj-ecDMJIg"
   },
   "source": [
    "### Выводы\n",
    "\n",
    "В этом задании вы научились решать арифметические действия с помощью языковой модели."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
