{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE_ArWZvZDTS"
   },
   "source": [
    "#Домашнее задание: реализация модели latent diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPRxOjA6ZQri"
   },
   "source": [
    "##Интро\n",
    "На семинарах мы познакомились с тем, как обучать диффузионную модель. Эти модели были реализованы через пиксельную диффузию, которую на практике уже никто не использует, потому что это вычислительно сложно обучить. На семинарах было разобрано обучение диффузионной модели. Для обучения вам предлагается обучение модели на датасете `nelorth/oxford-flowers` (https://huggingface.co/datasets/nelorth/oxford-flowers)  \n",
    "\n",
    "##Цель домашнего задания\n",
    "Научиться реализовать модель диффузии, модель скрытой диффузии и вносить в нее модификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AbSCRerDCdV"
   },
   "source": [
    "### [5 баллов] Реализация обучения диффузионной модели на новом датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM_oeT_JDY_Z"
   },
   "source": [
    "В этой части задания вам рекомендуется взять код с семинара и адаптировать его под обучение на новом датасете.\n",
    "\n",
    "**Ожидаемый результат**\n",
    "\n",
    "В качестве результатов модели, от вас требуется предоставить визуализации генераций, полученных от вашей модели и код обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torchvision) (2.1.0+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torch==2.1.0->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torch==2.1.0->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torch==2.1.0->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torch==2.1.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from torch==2.1.0->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install -q -U einops datasets matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting diffusers\n",
      "  Obtaining dependency information for diffusers from https://files.pythonhosted.org/packages/2e/ed/58a13f88dfcdd1bcfeabf44d4c9861979551339348e579fd6559c64b12e0/diffusers-0.21.4-py3-none-any.whl.metadata\n",
      "  Downloading diffusers-0.21.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (6.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (0.3.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (2023.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from importlib-metadata->diffusers) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.2->diffusers) (0.4.6)\n",
      "Downloading diffusers-0.21.4-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "    --------------------------------------- 0.0/1.5 MB 131.3 kB/s eta 0:00:12\n",
      "   - -------------------------------------- 0.0/1.5 MB 57.9 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.0/1.5 MB 57.9 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.1/1.5 MB 81.9 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.1/1.5 MB 81.9 kB/s eta 0:00:18\n",
      "   - -------------------------------------- 0.1/1.5 MB 85.5 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.1/1.5 MB 104.9 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.1/1.5 MB 121.3 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.1/1.5 MB 121.3 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.1/1.5 MB 121.3 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.1/1.5 MB 121.3 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.1/1.5 MB 121.3 kB/s eta 0:00:12\n",
      "   --- ------------------------------------ 0.1/1.5 MB 103.0 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.1/1.5 MB 103.0 kB/s eta 0:00:14\n",
      "   --- ------------------------------------ 0.1/1.5 MB 115.2 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 119.2 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 131.1 kB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 142.1 kB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 144.8 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 0.2/1.5 MB 152.9 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 0.2/1.5 MB 155.0 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 0.2/1.5 MB 155.0 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 0.2/1.5 MB 155.0 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 0.2/1.5 MB 155.0 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 0.2/1.5 MB 155.0 kB/s eta 0:00:09\n",
      "   ------ --------------------------------- 0.3/1.5 MB 147.0 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 0.3/1.5 MB 154.9 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.3/1.5 MB 155.2 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.3/1.5 MB 155.2 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 0.3/1.5 MB 159.7 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 0.3/1.5 MB 169.1 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.4/1.5 MB 174.1 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.4/1.5 MB 176.5 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.4/1.5 MB 176.5 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.4/1.5 MB 176.5 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.4/1.5 MB 176.5 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.4/1.5 MB 176.5 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 167.2 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 170.6 kB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 169.2 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 173.3 kB/s eta 0:00:07\n",
      "   ------------ --------------------------- 0.5/1.5 MB 179.5 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.5/1.5 MB 179.5 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 0.5/1.5 MB 179.8 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 0.5/1.5 MB 193.4 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 0.6/1.5 MB 209.4 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 223.5 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 232.1 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 232.1 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.6/1.5 MB 232.1 kB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 225.6 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 0.7/1.5 MB 237.0 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 0.8/1.5 MB 250.4 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 0.8/1.5 MB 259.4 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 0.8/1.5 MB 262.8 kB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 0.8/1.5 MB 268.1 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 0.9/1.5 MB 280.9 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.0/1.5 MB 302.2 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 322.9 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.1/1.5 MB 343.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 358.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 358.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.5 MB 355.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 359.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 386.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 411.6 kB/s eta 0:00:00\n",
      "Installing collected packages: diffusers\n",
      "Successfully installed diffusers-0.21.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (0.21.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (6.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (1.24.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from diffusers) (0.3.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (2023.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.2->diffusers) (23.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from importlib-metadata->diffusers) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->diffusers) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.2->diffusers) (0.4.6)\n",
      "Requirement already satisfied: transformers in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: datasets in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (0.17.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nekonn\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install diffusers\n",
    "!pip install diffusers -U\n",
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"sudo\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt -qq install git-lfs\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aCketD9tDYTC"
   },
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "\n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Lambda\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim import Adam\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel\n",
    "from transformers import  CLIPTokenizer\n",
    "from diffusers import AutoencoderKL\n",
    "from diffusers import UNet2DConditionModel\n",
    "from diffusers import PNDMScheduler\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "from huggingface_hub import Repository\n",
    "from huggingface_hub import whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем вспомогательные функции\n",
    "\n",
    "# Проверка на существование переменной\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "# Задать значение по умолчанию для переменной\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "# Модуль для реализации skip-connections (остаточных связей)\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "\n",
    "# Модуль для увеличения разрешения изображения\n",
    "def Upsample(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
    "\n",
    "# Модуль для уменьшения разрешения изображения\n",
    "def Downsample(dim):\n",
    "    return nn.Conv2d(dim, dim, 4, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модуль для генерации позиционных эмбеддингов на основе синусоид\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Базовый блок для сети\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups = 8):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "# Остаточный блок для сети ResNet\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.block1(x)\n",
    "\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
    "\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "# Блок ConvNext для архитектуры сети\n",
    "class ConvNextBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
    "\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
    "            if exists(time_emb_dim)\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
    "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, dim_out * mult),\n",
    "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.ds_conv(x)\n",
    "\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            assert exists(time_emb), \"time embedding must be passed in\"\n",
    "            condition = self.mlp(time_emb)\n",
    "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
    "\n",
    "        h = self.net(h)\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модуль внимания\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модуль линейного внимания\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                    nn.GroupNorm(1, dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модуль для нормализации перед применением функции активации\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основная архитектура сети U-Net\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim=None,\n",
    "        out_dim=None,\n",
    "        dim_mults=(1, 2, 4, 8),\n",
    "        channels=3,\n",
    "        with_time_emb=True,\n",
    "        resnet_block_groups=8,\n",
    "        use_convnext=True,\n",
    "        convnext_mult=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # определяем размерности\n",
    "        self.channels = channels\n",
    "\n",
    "        init_dim = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        if use_convnext:\n",
    "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
    "        else:\n",
    "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # time embeddings\n",
    "        if with_time_emb:\n",
    "            time_dim = dim * 4\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim),\n",
    "            )\n",
    "        else:\n",
    "            time_dim = None\n",
    "            self.time_mlp = None\n",
    "\n",
    "        # уровни\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        out_dim = default(out_dim, channels)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, time):\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
    "\n",
    "        h = []\n",
    "\n",
    "        # понижение дискретизации\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "\n",
    "        # узкое место\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        # повышение дискретизации\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            x = upsample(x)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Различные расписания для параметра beta\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    косинусный график как в https://arxiv.org/abs/2102.09672\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем количество временных шагов\n",
    "timesteps = 200\n",
    "\n",
    "# определяем beta schedule\n",
    "betas = linear_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "# определяем альфа\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# вычисления для диффузии q(x_t | x_{t-1}) и других\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# вычисления для постериорного q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "# Извлечение нужных значений из массива\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем размер изображения для дальнейших преобразований\n",
    "image_size = 128\n",
    "\n",
    "# Определение трансформаций для изображений\n",
    "transform = Compose([\n",
    "    Resize(image_size), # Изменяем размер изображения\n",
    "    CenterCrop(image_size), # Обрезаем изображение по центру\n",
    "    ToTensor(), # Преобразуем в torch.tensor с размерностью CHW и значениями в [0, 1]\n",
    "    Lambda(lambda t: (t * 2) - 1), # Сжимаем значения в диапазон [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обратные преобразования для изображений\n",
    "reverse_transform = Compose([\n",
    "     Lambda(lambda t: (t + 1) / 2), # Преобразуем значения в диапазон [0, 1]\n",
    "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW в HWC\n",
    "     Lambda(lambda t: t * 255.), # Преобразуем значения в диапазон [0, 255]\n",
    "     Lambda(lambda t: t.numpy().astype(np.uint8)), # Преобразуем тензор в numpy и изменяем тип на uint8\n",
    "     ToPILImage(), # Преобразуем в изображение PIL\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для прямой диффузии\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения зашумленного изображения\n",
    "def get_noisy_image(x_start, t):\n",
    "    # Добавляем шум\n",
    "    x_noisy = q_sample(x_start, t=t)\n",
    "    \n",
    "    # Возвращаем к формату PIL image\n",
    "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем seed для воспроизводимости\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Функция для визуализации списка изображений\n",
    "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # делаем 2d сетку, даже если это 1 строка\n",
    "        imgs = [imgs]\n",
    "\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0]) + with_orig\n",
    "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        row = [image] + row if with_orig else row\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    if with_orig:\n",
    "        axs[0, 0].set(title='Original image')\n",
    "        axs[0, 0].title.set_size(8)\n",
    "    if row_title is not None:\n",
    "        for row_idx in range(num_rows):\n",
    "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция потерь для декодера\n",
    "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "\n",
    "    if loss_type == 'l1':\n",
    "        loss = F.l1_loss(noise, predicted_noise)\n",
    "    elif loss_type == 'l2':\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "    elif loss_type == \"huber\":\n",
    "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет\n",
    "dataset = load_dataset('nelorth/oxford-flowers')\n",
    "\n",
    "# Выводим датасет train и test\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Итерация по датасету\n",
    "for example in train_dataset:\n",
    "    image = example[\"image\"]\n",
    "    label = example[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаем параметры\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128\n",
    "\n",
    "# Определение трансформаций для изображений\n",
    "transform = Compose([\n",
    "            transforms.Resize((image_size, image_size)), # Изменение размера изображения\n",
    "            transforms.RandomHorizontalFlip(), # Случайное горизонтальное отражение\n",
    "            transforms.ToTensor(), # Преобразование в тензор\n",
    "            transforms.Lambda(lambda t: (t * 2) - 1) # Нормализация в диапазон [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values'])\n"
     ]
    }
   ],
   "source": [
    "# Функция преобразования изображений\n",
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
    "\n",
    "# Создание DataLoader\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для прямой диффузии изображения\n",
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итерация по временным шагам для диффузии\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    b = shape[0]\n",
    "    # начинаем с чистого шума (для каждого примера в батче)\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
    "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "    return imgs\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, image_size, batch_size=16, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "# Создаем папку для сохранения результатов\n",
    "results_folder = Path(\"./results\")\n",
    "results_folder.mkdir(exist_ok = True)\n",
    "save_and_sample_every = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение устройства для обучения (GPU или CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Инициализация модели и перемещение на устройство\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4,)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Определение оптимизатора\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878a2a8fdea44b69af0ccdd1b9672346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.44348400831222534\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218e4fc90c004d049fdeabb4f3980fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.14315466582775116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cc7faeed8a40118438c455c1928a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.09774802625179291\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd8f7ad481b4aa3a18e691de08a5c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08707530796527863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113548b958584d80895304114745027d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0967123806476593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b767db145c745c3bbdd2fa604dccecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08042284101247787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25b6e7fc4ba441dab848b14fe9adca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08148293197154999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64db9526b5344bf2a13a8aabcc961616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08189857751131058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7706cbcd9fd42f8b420e39d18e26926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05380536988377571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0653f61ac7c466287f69d89326cd2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08881264925003052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03370286c8f1434b8f0ff881b17e4e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07935944199562073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79613360de7d4618ba4f93cbfcfdf941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06467163562774658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f09e801183a4d54a76b0bc0ea66a714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0639226958155632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b45f9c8492843fc9b1d128cf1c165f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06550668179988861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5843ee43f9a34fa8acfef1dfebe8cfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0759468823671341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610f92b58e8244d59b4e1e3304dac449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08499535918235779\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e67c8014e6c486aaf3381641fc3488f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07486893236637115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61381dd427354380884edec95ff1bd7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.056905876845121384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec048eb230894fdca6103f633d98ee40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06955718994140625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b856a508ee3e449897b91cad3cafd828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0736919566988945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9651472654ea4a0f8a5eaf1ffec1c206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.08556445688009262\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f664df460feb423dbe86fa732e69dbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07561871409416199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6417ca60bb6b46809e5e1778a6180ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07830718159675598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adcb52f3eedd481388df8bc2dd00e525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06851845979690552\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cbed9a33d64247b41d94e3bdcd7c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07833462208509445\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27875d82aca4c92824077361d2c289b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06215383857488632\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acd08325606433faf05ba2bcd6614bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06863363087177277\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a65f8c93197476cbc5b0f1c4707aecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.05986873805522919\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d27d33dfc947b79db9cfd83555257a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06435395032167435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a96efac0e5049ebb3859b38b06e3edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/30:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.07110165059566498\n"
     ]
    }
   ],
   "source": [
    "# Количество эпох обучения\n",
    "epochs = 30\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(epochs):\n",
    "    # Обертываем dataloader с помощью tqdm для отображения индикатора прогресса\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = batch[\"pixel_values\"].shape[0]\n",
    "        batch = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "        # Случайная выборка временных шагов для каждого изображения в батче\n",
    "        t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "        # Расчет функции потерь\n",
    "        loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
    "\n",
    "        # Вывод значения функции потерь каждые 100 шагов\n",
    "        if step % 100 == 0:\n",
    "            print(\"Loss:\", loss.item())\n",
    "\n",
    "        # Обратное распространение ошибки\n",
    "        loss.backward()\n",
    "\n",
    "        # Шаг оптимизации\n",
    "        optimizer.step()\n",
    "\n",
    "        # Сохранение сгенерированных изображений каждые 1000 шагов\n",
    "        if step != 0 and step % save_and_sample_every == 0:\n",
    "            milestone = step // save_and_sample_every\n",
    "            batches = num_to_groups(4, batch_size)\n",
    "            all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
    "            all_images = torch.cat(all_images_list, dim=0)\n",
    "            all_images = (all_images + 1) * 0.5\n",
    "            save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWIrC61EDxYV"
   },
   "source": [
    "Ваши визуализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot([get_noisy_image(x_start, torch\u001b[38;5;241m.\u001b[39mtensor([t])) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m199\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot([get_noisy_image(x_start, torch\u001b[38;5;241m.\u001b[39mtensor([t])) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m199\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_start' is not defined"
     ]
    }
   ],
   "source": [
    "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T75381b6DzmZ"
   },
   "outputs": [],
   "source": [
    "# # сэмплирует 64 изображения\n",
    "# samples = sample(model, image_size=image_size, batch_size=16, channels=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # показывает один случайный\n",
    "# random_index = 10\n",
    "# plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # создаем gif процесса шумоподавления:\n",
    "# random_index = 53\n",
    "\n",
    "# ims = []\n",
    "# for i in range(timesteps):\n",
    "#     t = np.uint8((samples[i][random_index].clip(-1, 1) + 1) / 2 * 255)\n",
    "#     im = Image.fromarray(t.squeeze(0)).resize((512, 512)).convert('P')\n",
    "#     ims.append(im)\n",
    "\n",
    "\n",
    "# ims[0].save('diffusion.gif',\n",
    "#                save_all=True, append_images=ims[1:], optimize=False, duration=40, loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrPQbkMzDLJJ"
   },
   "source": [
    "### [3 балла] Реализация обучения модели latent diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGaIuk_mX20d"
   },
   "source": [
    "Напоминание про реализацию модели [Latent Diffusion](https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html)\n",
    "\n",
    "Для того, чтобы из кода обучения Diffusion получить обучения Latent Diffusion, вам нужно добавить VQ-VAE и заморозить! и обрабатывать изображения с помощью `vae.encode`. Далее полученные latent, подаются в стандартный пайплайн обучения. Учтите, что latent имеет меньшую размерность и вам может понадобиться уменьшить число слоёв в архитектуре UNet.\n",
    "\n",
    "В качестве модели VQ-VAE вы можете воспользоваться предобученным VQ-VAE от Stable diffusion.\n",
    "\n",
    "**Ожидаемый результат**\n",
    "\n",
    "В качестве результатов модели, от вас требуется предоставить визуализации генераций, полученных от вашей модели, и код обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация и вход в аккаунт \n",
    "notebook_login()\n",
    "\n",
    "# Загрузка предварительно обученной модели DDPM для CelebA-HQ (размер изображения 256x256) и перенос на GPU\n",
    "ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-celebahq-256\").to(\"cuda\")\n",
    "\n",
    "# Генерация изображения с использованием модели DDPM (с 25 шагами вывода)\n",
    "image = ddpm(num_inference_steps=25).images[0]\n",
    "\n",
    "# Отображение сгенерированного изображения\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка шагов для предварительно обученной модели\n",
    "scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-celebahq-256\")\n",
    "\n",
    "# Загрузка архитектуры UNet2D для предварительно обученной модели и перенос на GPU\n",
    "model = UNet2DModel.from_pretrained(\"google/ddpm-celebahq-256\").to(\"cuda\")\n",
    "\n",
    "# Установка числа временных шагов для планировщика\n",
    "scheduler.set_timesteps(50)\n",
    "\n",
    "# Вывод доступных временных шагов\n",
    "scheduler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение размеров изображения и создание начального шума\n",
    "sample_size = model.config.sample_size\n",
    "noise = torch.randn((1, 3, sample_size, sample_size)).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = noise\n",
    "\n",
    "# Проход по всем временным шагам для генерации изображения\n",
    "for t in scheduler.timesteps:\n",
    "    with torch.no_grad():\n",
    "        noisy_residual = model(input, t).sample\n",
    "    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n",
    "    input = previous_noisy_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конвертация генерированного изображения в формат PIL\n",
    "image = (input / 2 + 0.5).clamp(0, 1)\n",
    "image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "image = Image.fromarray((image * 255).round().astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теперь начнем писать код для SD (Stable Difussion)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192,
     "referenced_widgets": [
      "8412794934e64aeaa6d5586ae98b190c",
      "b5dc327087da4d12b8c23ac4e1845a5d",
      "9fb86eab7d7a47b89aac261a3db3eaba",
      "ca6b276b461541b7a132a65c1864a7e9",
      "ad32733a6cab4f88897a1372564a677c",
      "b4ad43043ca0454ab68004e5cba77bae",
      "c98e60b857004dbe9feefa2ed04c32a7",
      "66ce607865384e2eb495808086252ef7",
      "483243857ab94ea0ab5cac4443b23c1a",
      "7f2f877ea742453c831bdb69a32c8b90",
      "630dc2755ce745989abeeeea1c696f53",
      "5bef65ea29524ed299c2033c2f9e622b",
      "70148df5a7534be6b7e2aab3d2f119c5",
      "2c213a32ec484346be2f0efbf92b29a8",
      "5b20e4a56cf245e0b4d8a21da7ad9dd0",
      "194e401f1b364ababdd68f6d75dfe1ed",
      "1c3ac5689c7b4f4aa33207e52cee853e",
      "a300d22acb424278a33780c8f34ff0d4",
      "efad75ccca0a41cbada8d639d34deee1",
      "c969985292f54387be2da9eb02f3da77",
      "080ef73a1aef472681987a7379d90458",
      "0deef98556834ad8ad3e9c41676ee607"
     ]
    },
    "id": "gr8ZcRIKVhq_",
    "outputId": "03692562-d703-4848-feca-8bad39e15fa9"
   },
   "outputs": [],
   "source": [
    "# Загрузка дополнительных компонентов для генерации изображений с условиями\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем другой планировщик\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n",
    "\n",
    "# Переносим модели на GPU\n",
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замораживаем VAE и text_encoder для обучения\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "unet.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфигурация для обучения\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # сгенерированное разрешение изображения\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # сколько изображений мы можем сэмплировать во время eval\n",
    "    num_epochs = 5\n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "    save_image_epochs = 1\n",
    "    save_model_epochs = 1\n",
    "    mixed_precision = 'fp16'  # `no` для float32, `fp16` для автоматической смешанной точности\n",
    "    output_dir = 'ddpm-butterflies-128'  # имя модели локально и на HF Hub\n",
    "\n",
    "    push_to_hub = False  # Если True, то загружаем сохраненную модель в HF Hub\n",
    "    hub_private_repo = False\n",
    "    overwrite_output_dir = True  # Если True, то перезаписываем старую модель при повторном запуске ноутбука\n",
    "    seed = 0\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение текстового запроса для генерации изображения\n",
    "prompt = [\"A cinematic shot of a Batman, filmed on Red V‑Raptor 8K camera, 50 mm, f/1.4, film director David Fincher, chiaroscuro, full‑body, futuristic cyberpunk costume, magic realism.\"]\n",
    "height = 512  # высота Stable Diffusion по умолчанию\n",
    "width = 512  # ширина Stable Diffusion по умолчанию\n",
    "num_inference_steps = 25  # Количество шагов шумоподавления\n",
    "guidance_scale = 7.5  # Масштаб classifier-free guidance\n",
    "generator = torch.manual_seed(0)  # Seed generator чтобы создать начальный скрытый шума\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация текстового запроса\n",
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Получение текстовых эмбеддингов\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "\n",
    "# Токенизация пустой строки для получения \"базового\" эмбеддинга\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "# Комбинация базовых и текстовых эмбеддингов\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка на соответствие размерности\n",
    "2 ** (len(vae.config.block_out_channels) - 1) == 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание начального скрытого состояния\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ")\n",
    "latents = latents.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = latents * scheduler.init_noise_sigma\n",
    "# Установка количества временных шагов для планировщика\n",
    "scheduler.set_timesteps(num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проход по всем временным шагам для генерации изображения\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # Расширение скрытого состояния для руководства\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    # Масштабирование входа модели\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # Предсказание остаточного шума\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # Применение руководства без классификатора\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # Обратный шаг во времени для получения предыдущего шумного изображения\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Масштабирование и декодирование скрытого состояния с помощью VAE\n",
    "latents = 1 / 0.18215 * latents\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование и нормализация изображения\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "\n",
    "# Преобразование массива в объекты PIL Image\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "# Вывод сгенерированного изображения\n",
    "pil_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теперь обучаем свою сеть**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size = 128  # сгенерированное разрешение изображения\n",
    "    train_batch_size = 16  # размер батча во время обучения\n",
    "    eval_batch_size = 16  # размер батча во время оценки (сколько изображений мы можем сэмплировать во время eval)\n",
    "    num_epochs = 7  # количество эпох обучения\n",
    "    gradient_accumulation_steps = 1  # количество шагов для накопления градиента\n",
    "    learning_rate = 1e-4  # скорость обучения\n",
    "    lr_warmup_steps = 500  # количество шагов для \"разогрева\" скорости обучения\n",
    "    save_image_epochs = 1  # через сколько эпох сохранять изображения\n",
    "    save_model_epochs = 1  # через сколько эпох сохранять модели\n",
    "    mixed_precision = 'fp16'  # используемая точность (`no` для float32, `fp16` для автоматической смешанной точности)\n",
    "    output_dir = 'ddpm-butterflies-128'  # путь для сохранения модели\n",
    "\n",
    "    # настройки для загрузки модели в HF Hub\n",
    "    push_to_hub = False  # если True, то загружаем сохраненную модель в HF Hub\n",
    "    hub_private_repo = False  # если True, репозиторий будет приватным\n",
    "    overwrite_output_dir = True  # если True, перезаписываем существующую модель при повторном запуске\n",
    "    seed = 0  # начальное значение для генератора случайных чисел\n",
    "\n",
    "# Создание экземпляра конфигурации\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка датасета\n",
    "config.dataset_name = \"huggan/pokemon\"\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация изображений из датасета\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[5:9][\"image\"]):\n",
    "    axs[i].imshow(image)\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение препроцессинга для изображений\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((config.image_size, config.image_size)),  # изменение размера изображения\n",
    "        transforms.RandomHorizontalFlip(),  # случайное горизонтальное отражение\n",
    "        transforms.ToTensor(),  # преобразование изображения в тензор\n",
    "        transforms.Normalize([0.5], [0.5]),  # нормализация изображения\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Функция для преобразования изображений\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "# Применение препроцессинга к датасету\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация преобразованных изображений\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, image in enumerate(dataset[5:9][\"images\"]):\n",
    "    axs[i].imshow(image.permute(1, 2, 0).numpy() / 2 + 0.5)\n",
    "    axs[i].set_axis_off()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание даталоадера для обучения\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение модели UNet\n",
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # разрешение изображения\n",
    "    in_channels=3,  # количество входных каналов (3 для RGB изображений)\n",
    "    out_channels=3,  # количество выходных каналов\n",
    "    layers_per_block=2,  # количество ResNet слоев в каждом блоке UNet\n",
    "    block_out_channels=(128, 128, 256, 256, 512, 512),  # количество выходных каналов для каждого блока UNet\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # стандартный блок снижения разрешения\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # блок снижения разрешения с механизмом внимания\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # стандартный блок повышения разрешения\n",
    "        \"AttnUpBlock2D\",  # блок повышения разрешения с механизмом внимания\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка размеров входного и выходного изображения для модели\n",
    "sample_image = dataset[0]['images'].unsqueeze(0)\n",
    "print('Input shape:', sample_image.shape)\n",
    "print('Output shape:', model(sample_image, timestep=0).sample.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0xyTjivGUlC"
   },
   "source": [
    "### [2 балла] Анализ влияния scheduler на качество генерации\n",
    "\n",
    " На качество генерации может влиять используемый scheduler и число шагов, поэтому в этой части домашнего задания, вам предлагается происследовать визуально как scheduler и число шагов влияют на качество генерации. Вам предлагается рассмотреть качество генерации на 10 различных числах шагов и,  используя различные scheduler.\n",
    "\n",
    "\n",
    " В качестве scheduler предлагается стандартный DDPM, DDIM, DPMSolver (Другие scheduler не воспрещаются). Обратите внимание, что DDIM и DPMSolver являются детерминированными scheduler, а значит результат у них должен получаться одиннаковый.\n",
    "\n",
    " **Ожидаемый результат**\n",
    "\n",
    " В качестве результатов в этом пункте от вас требуется предоставить визуализации картинок в различных вариации числа шагов и scheduler и сделать выводы, какие параметры лучше использовать. Вы можете выполнить эту часть, не используя обученную выше модель, а используя стандартные pipeline StableDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYBOdjpyG1kA"
   },
   "outputs": [],
   "source": [
    "# Определение шумового расписания для модели\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# Генерация случайного шума и добавление его к изображению\n",
    "noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "# Визуализация зашумленного изображения\n",
    "Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание модели для зашумленного изображения\n",
    "noise_pred = model(noisy_image, timesteps).sample\n",
    "\n",
    "# Расчет функции потерь на основе разницы между предсказанным и истинным шумом\n",
    "loss = F.mse_loss(noise_pred, noise)\n",
    "\n",
    "# Определение оптимизатора для обучения модели\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение косинусной скорости обучения\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания сетки из изображений\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "# Функция для оценки модели после каждой эпохи обучения\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Сэмплирование изображений из случайного шума\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.manual_seed(config.seed),\n",
    "    ).images\n",
    "\n",
    "    # Сборка сетки из сэмплированных изображений\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Сохранение сетки изображений\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_device = 'cuda'\n",
    "# Перемещение модели на устройство для обучения (GPU или CPU)\n",
    "model = model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение полного имени репозитория для загрузки модели на HF Hub\n",
    "def get_full_repo_name(model_id: str, organization: str = None, token: str = None):\n",
    "    if token is None:\n",
    "        token = HfFolder.get_token()\n",
    "    if organization is None:\n",
    "        username = whoami(token)[\"name\"]\n",
    "        return f\"{username}/{model_id}\"\n",
    "    else:\n",
    "        return f\"{organization}/{model_id}\"\n",
    "\n",
    "# Главный цикл обучения модели\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    global_step = 0\n",
    "\n",
    "    # Начало процесса обучения\n",
    "    for epoch in tqdm(range(config.num_epochs), desc=\"Epochs\"):\n",
    "        progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images'].to(torch_device)\n",
    "            \n",
    "            # Генерация шума для добавления к чистым изображениям\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Случайный выбор временных шагов для каждого изображения в пакете\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Добавление шума к чистым изображениям\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            # Предсказание модели для зашумленных изображений\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "\n",
    "            # Расчет функции потерь и обновление весов модели\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        # Оценка модели после каждой эпохи\n",
    "        pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            evaluate(config, epoch, pipeline)\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            pipeline.save_pretrained(config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения модели\n",
    "train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание пайплайна для модели после обучения\n",
    "pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "\n",
    "# Оценка обученной модели\n",
    "evaluate(config, 0, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация сгенерированных изображений после обучения\n",
    "sample_images = sorted(glob.glob(f\"{config.output_dir}/samples/*.png\"))\n",
    "Image.open(sample_images[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1aVf3JyY8iW"
   },
   "source": [
    "### Дополнительное задание [дополнительный 1 балл]. Модификации conditional guidance*\n",
    "*Дополнительный балл не умножается на коэффициент домашнего задания*\n",
    "\n",
    "---\n",
    "\n",
    "Далее, вам предлагается реализовать conditional guidance на этом датасете. Как уже упоминалось на лекции механизм conditional guidance позволяет вам генерировать изображения с выбранными вами условиями (картинки определенных цветков, например). На семинаре 2 был показан участок кода, который реализовал условную генерацию, однако для этого вам нужно добавить прокидывание условия в UNet и чуть поменять процесс условной генерации. При реализации вы можете опираться адаптировать следующую [реализацию](https://github.com/TeaPearce/Conditional_Diffusion_MNIST).\n",
    "\n",
    "**Ожидаемый результат**\n",
    "\n",
    "В качестве результатов модели, от вас требуется предоставить визуализации генераций, полученных от вашей модели, и код обучения модели.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "080ef73a1aef472681987a7379d90458": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0deef98556834ad8ad3e9c41676ee607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "194e401f1b364ababdd68f6d75dfe1ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c3ac5689c7b4f4aa33207e52cee853e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c213a32ec484346be2f0efbf92b29a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efad75ccca0a41cbada8d639d34deee1",
      "max": 334643276,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c969985292f54387be2da9eb02f3da77",
      "value": 334643276
     }
    },
    "483243857ab94ea0ab5cac4443b23c1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b20e4a56cf245e0b4d8a21da7ad9dd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_080ef73a1aef472681987a7379d90458",
      "placeholder": "​",
      "style": "IPY_MODEL_0deef98556834ad8ad3e9c41676ee607",
      "value": " 335M/335M [00:03&lt;00:00, 138MB/s]"
     }
    },
    "5bef65ea29524ed299c2033c2f9e622b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_70148df5a7534be6b7e2aab3d2f119c5",
       "IPY_MODEL_2c213a32ec484346be2f0efbf92b29a8",
       "IPY_MODEL_5b20e4a56cf245e0b4d8a21da7ad9dd0"
      ],
      "layout": "IPY_MODEL_194e401f1b364ababdd68f6d75dfe1ed"
     }
    },
    "630dc2755ce745989abeeeea1c696f53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66ce607865384e2eb495808086252ef7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70148df5a7534be6b7e2aab3d2f119c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c3ac5689c7b4f4aa33207e52cee853e",
      "placeholder": "​",
      "style": "IPY_MODEL_a300d22acb424278a33780c8f34ff0d4",
      "value": "Downloading (…)ch_model.safetensors: 100%"
     }
    },
    "7f2f877ea742453c831bdb69a32c8b90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8412794934e64aeaa6d5586ae98b190c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5dc327087da4d12b8c23ac4e1845a5d",
       "IPY_MODEL_9fb86eab7d7a47b89aac261a3db3eaba",
       "IPY_MODEL_ca6b276b461541b7a132a65c1864a7e9"
      ],
      "layout": "IPY_MODEL_ad32733a6cab4f88897a1372564a677c"
     }
    },
    "9fb86eab7d7a47b89aac261a3db3eaba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_66ce607865384e2eb495808086252ef7",
      "max": 551,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_483243857ab94ea0ab5cac4443b23c1a",
      "value": 551
     }
    },
    "a300d22acb424278a33780c8f34ff0d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad32733a6cab4f88897a1372564a677c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4ad43043ca0454ab68004e5cba77bae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5dc327087da4d12b8c23ac4e1845a5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4ad43043ca0454ab68004e5cba77bae",
      "placeholder": "​",
      "style": "IPY_MODEL_c98e60b857004dbe9feefa2ed04c32a7",
      "value": "Downloading (…)main/vae/config.json: 100%"
     }
    },
    "c969985292f54387be2da9eb02f3da77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c98e60b857004dbe9feefa2ed04c32a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca6b276b461541b7a132a65c1864a7e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f2f877ea742453c831bdb69a32c8b90",
      "placeholder": "​",
      "style": "IPY_MODEL_630dc2755ce745989abeeeea1c696f53",
      "value": " 551/551 [00:00&lt;00:00, 5.94kB/s]"
     }
    },
    "efad75ccca0a41cbada8d639d34deee1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
