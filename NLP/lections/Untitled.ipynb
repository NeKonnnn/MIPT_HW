{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d60caad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments'],\n",
       "    num_rows: 11514\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"AmazonScience/massive\", \"en-US\", split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ee81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b84c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a221da0cf54f959663f0463f6df799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1', 'locale': 'en-US', 'partition': 'train', 'scenario': 16, 'intent': 48, 'utt': 'wake me up at nine am on friday', 'annot_utt': 'wake me up at [time : nine am] on [date : friday]', 'worker_id': '1', 'slot_method': {'slot': [], 'method': []}, 'judgments': {'worker_id': [], 'intent_score': [], 'slots_score': [], 'grammar_score': [], 'spelling_score': [], 'language_identification': []}, 'utt_tokenized': ['wake', 'me', 'up', 'at', 'nine', 'am', 'on', 'friday']}\n"
     ]
    }
   ],
   "source": [
    "# Создайте функцию токенизации\n",
    "def tokenize_text(text):\n",
    "    if pd.notnull(text):\n",
    "        return word_tokenize(text)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Примените функцию токенизации к каждой строке в столбце 'utt'\n",
    "dataset = dataset.map(lambda x: {'utt_tokenized': tokenize_text(x['utt'])})\n",
    "\n",
    "# Просмотрите первые несколько строк датасета, чтобы убедиться, что токенизация работает правильно\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30457dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aapa' 'aaron' 'abdul' 'abita' 'able' 'abolish' 'about' 'above'\n",
      " 'abraham']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Преобразование набора данных в список строк\n",
    "documents = [x['utt'] for x in dataset if x['utt'] is not None]\n",
    "\n",
    "# Создание объекта TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Применение TF-IDF векторизатора к набору данных\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Печать первых нескольких признаков\n",
    "print(vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Печать первых нескольких строк TF-IDF матрицы\n",
    "print(X.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
