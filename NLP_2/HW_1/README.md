Сбор данных:
Dataset был взят с Kaggle: https://www.kaggle.com/datasets/zerofactorialisone/rick-and-morty-discord-chatbot-dataset и в ходе разработки моделей претерпел некоторые изменения! Изначально датасет был на английском языке, но я захотел, чтобы мой бот Рик был русскоязычным, поэтому, для того чтобы сократить время на перевод я воспользовался нейронной сетью DeepL Translate, и сохранил результат в docx формат. Далее была проведена предобработка данных в ноутбуке EDA.ipynb. Для установления характера Рика, я выяснил, какими словами он пользуется чаще всего, а также обратил внимание на манеру его речи и ведение диалога! После этого сформировал итоговый датасет, который находится в папке data\data_OUT\rick.csv

Разработка модели:
При разработке было построено 2 модели:
Первая модель была построена на принципе bag_of_words и содержится в bag_of_words_model.ipynb, а вторая модель была построена на основе cross_encoder и содержится в файле cross_encoder_model.py. Была взята за основу предобученная модель bert-base-uncased.
Результаты обучения второй модели: 
 
Выводы по результатам модели:
Данные результаты указывают на очень высокую эффективность модели в процессе обучения и валидации. Стабильная точность 100% на обучающем и валидационном наборах данных может свидетельствовать о следующем:
1.	Переобучение: модель идеально подстроилась под тренировочные данные, что может привести к снижению ее способности к обобщению на новых данных. Однако стабильно высокая точность на валидационном наборе может говорить о том, что данные хорошо представляют общую закономерность, которую модель смогла выучить.
2.	Качество данных: возможно, датасет не достаточно разнообразен или слишком мал, что позволяет модели легко достичь высокой точности. В таком случае, важно обеспечить большую вариативность и объем обучающих данных.
Я склонен придерживаться второго мнения, т.к. итоговый набор данных действительно не большой. В силу отсутствия времени больший набор собрать не удалось!
Код инференса модели:
 
При тестировании модели на примере вопроса "Как тебя зовут?", модель смогла с высокой уверенностью (98%) выбрать подходящий ответ, что свидетельствует о хорошей способности модели обрабатывать естественный язык и выдавать релевантные ответы на поставленные вопросы.

Ускорение инференса модели:
 
После процесса квантования модели, точность на валидационном наборе данных осталась стабильно высокой (100%). Это говорит о том, что квантование не оказало заметного негативного влияния на производительность модели, что является значительным достижением, учитывая сокращение занимаемых ресурсов и увеличение скорости инференса.
 

Запуск бота:
Данный процесс реализован через FastAPI.
Чтобы запустить бота, нужно:
1. установить ngrok.exe с сайта https://ngrok.com/ локально на ПК
2. зарегистрироваться
3. после установки в появившемся окне Your Authtoken
4. в файле settings.py WEBHOOK_HOST на ваш token
5. Далее запустить файл my_main.py

После этого можно найти в телеграмме бота rick_pickle_bot и общаться с ним!

 

Заключение
На основе проведенного анализа можно сделать вывод о высокой эффективности разработанной модели для задачи автоматизированного чат-бота. Однако для обеспечения более глубокого понимания ее способностей и ограничений необходимо провести дополнительные эксперименты, включая тестирование на более разнообразном и объемном наборе данных, а также оценку способности модели к обобщению на новых, невиданных примерах.

