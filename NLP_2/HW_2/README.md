# Генеративный чатбот Рик!
![Rick](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/36d11c49-e839-4163-9e5f-5089833e42e9)

# Сбор данных:
Dataset был взят с Kaggle: https://www.kaggle.com/datasets/zerofactorialisone/rick-and-morty-discord-chatbot-dataset и в ходе разработки моделей претерпел некоторые изменения! Изначально датасет был на английском языке, но я захотел, чтобы мой бот Рик был русскоязычным, поэтому, для того чтобы сократить время на перевод я воспользовался нейронной сетью DeepL Translate, и сохранил результат в docx формат. Далее была проведена предобработка данных в ноутбуке EDA.ipynb. Для установления характера Рика, я выяснил, какими словами он пользуется чаще всего, а также обратил внимание на манеру его речи и ведение диалога! После этого сформировал итоговый датасет, который находится в папке data\data_OUT\rick.csv
![rick_wordcloud](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/b85ee4c8-4586-46bb-831d-c7e4443f0900)

# Разработка модели:
При разработке была построена генеративная в стиле персонажа Рика из мультсериала "Рик и Морти". Разработка модели включает в себя несколько ключевых этапов:
Предобработка данных: Используется функция preprocess_text_improved для очистки текста от специальных символов, за исключением знаков пунктуации. Такой подход позволяет сохранить семантику предложений и избежать потери важной информации. Данные загружаются из CSV-файла, после чего вопросы и ответы объединяются в один текст с разделителем для последующей обработки.
Инициализация и обучение модели: В качестве основы выбрана предобученная модель sberbank-ai/rugpt3large_based_on_gpt2 от Sberbank AI. Эта модель адаптирована для русского языка, что делает её подходящим выбором для работы с текстами на русском языке. Производится токенизация текстов и их подготовка к обучению с помощью специализированных классов из библиотеки transformers. Обучение модели настраивается через класс TrainingArguments, где указываются параметры обучения, такие как количество эпох, размер батча, стратегия валидации и др.
## Результаты обучения второй модели: 
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/8eb04667-0552-4eae-9f5f-d581261186a8)
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/2e20518e-5465-40ed-abb9-6cd23ba7584b)

## Выводы по результатам модели:
Данные результаты указывают на очень высокую эффективность модели в процессе обучения и валидации. 
Значения потерь как на тренировочном, так и на валидационном наборах данных уменьшаются с каждой эпохой, что указывает на то, что модель успешно учится и адаптируется к задаче. Снижение потерь говорит о том, что модель становится все лучше в предсказании правильных ответов. Значительное уменьшение потерь на тренировочной выборке с первой по третью эпоху (с 3.34 до 1.75) свидетельствует о хорошем обучении модели. Это указывает на то, что модель успешно извлекает закономерности из обучающих данных и эффективно обобщает эти знания. Потери на валидационной выборке также снижаются, но более плавно, чем на тренировочной. Это естественно, поскольку валидационный набор данных используется для оценки обобщающей способности модели на данных, которые она ранее не видела. Снижение валидационных потерь с 2.47 до 2.17 подтверждает, что модель не переобучается и хорошо обобщает знания на новых данных.
Наблюдаемая разница между тренировочными и валидационными потерями уменьшается с течением времени, что является положительным индикатором. Однако важно следить за этим различием: если оно становится слишком маленьким, это может указывать на недообучение, а если слишком большим — на переобучение.
Исходя из всего выше сказанного, можно подытожить, что уменьшение валидационных потерь и сближение их значений с тренировочными свидетельствует о хорошей обобщающей способности модели. Это говорит о том, что модель способна адекватно реагировать на новые данные, что является ключевым для задач генерации текста.

# Код инференса модели:
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/185e0791-24b5-44cd-b67d-54025a93482a)
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/3d2eb832-0daa-4cad-ab57-7e7315ff35f5)

На скриншоте заметно, что можно после ответа обозначить похож ли ответ на стилистику персонажа Рика из м/с "Рик и Морти". В самом боте этот функционал отключил для упрощения, но можно подключить его, если выкатывать в пром для сбора обратной связи, расширения датасета и дальнейшего дообучения модели!

# Применение методов улучшения фактологической связности ответа:
Для улучшения фактологической связанности ответа были использованы контекстно-зависимык эмбеддинги. В данном случае, мной была применена модель t5-large, для помощи в улучшении понимания контекста и генерации более точных ответов. Эта модель лучше улавливает семантическую связь между словами в тексте и может использоваться для предобработки входных данных или как часть ансамбля моделей.
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/dd918a38-3d51-46aa-b951-f377c6835c2c)

# Запуск бота:
Данный процесс реализован через FastAPI.
Чтобы запустить бота, нужно:
1. установить ngrok.exe с сайта https://ngrok.com/ локально на ПК
2. зарегистрироваться
3. после установки в появившемся окне Your Authtoken
4. в файле settings.py WEBHOOK_HOST на ваш token
5. Далее запустить файл my_main.py

После этого можно найти в телеграмме бота rick_pickle_bot и общаться с ним!
![image](https://github.com/NeKonnnn/MIPT_HW/assets/91149797/054d7b14-cb91-4096-9968-468f989adbb0)

# Заключение
На основе проведенного анализа можно сделать вывод о высокой эффективности разработанной модели для задачи генеравтивного чат-бота. Однако для обеспечения более глубокого понимания ее способностей и ограничений необходимо провести дополнительные эксперименты, включая тестирование на более разнообразном и объемном наборе данных, а также оценку способности модели к обобщению на новых, невиданных примерах.


